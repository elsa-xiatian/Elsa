@[TOC]
# 概述

本篇文章是基于小林coding的计算机网络整理总结而成（没有系统学习过或者学的不清楚的很建议看一遍小林，讲的真的很好！），在本篇文章中我选取了我在学习过程中觉得很重要及比较困难和系统的知识点并以问题的方式列出。希望能够帮助到也在学计网的你！并恳请大家指出文章中我理解不当的地方！

# 基础
## 1.TCP/IP的网络模型是怎样的的？每一层的功能如何？
首先我们为什么需要这样的网络模型？原因很简单：就像车同轨、书同文一样参与网络通信的设备同样也是多种多样的，为了兼容各种设备，就协商出了一套通用的网络协议，只要大家都按照规定的协议编写开发就能很容易的实现互联。并且这个协议是分层的，每一层都有各自需要负责的业务（为什么要分层？我个人的理解是这样的：就像业务开发一样，为了追求更高的可读性以及便于维护，我们常常会将功能相似的代码抽象成一个类，每个类只需要专注处理好自己的业务就好，至于调用到的其他方法并不需要关心他的具体实现，只需要直接处理方法的结果就好）。下面将就TCP/IP网络模型分别对每一层进行介绍。

### 1.1 应用层
他是整个网络模型中最上层的，也是我们能够直接接触到的，他主要的作用就是为用户提供各种网络应用服务：常见的比如用户交互（游戏中输入指令，在服务器处理完后展示给我们的已更新的游戏状态）、数据处理（将我们在浏览器中键入的文本转化成HTTP请求以及对数据加密处理等）、协议支持（为了实现各种各样的网络功能，应用层也需要遵守各类协议，如：HTTP、SMTP等）。

那具体涉及到的数据是怎样传输的？那不是我们这层该管的事，只需要把数据包交给下一层：传输层，让他去处理就好了。
### 1.2 传输层
如上文所述，这层的主要功能就是为应用层提供网络支持。同样也实现这样的功能也需要遵守协议：TCP、UDP。TCP为数据的正确传输提供了可靠的保障（怎么保障的下面会提到，这里先略过）。

UDP的实现则相对比较简单，他更关心怎么将数据包发送，至于别的就先忽略，正是这样的特性决定了他的传输效率以及时效性都更高（当然UDP也是可以实现可靠传输的，具体实现后文会提到）。
![图源小林！！！](https://i-blog.csdnimg.cn/direct/c6b44e3a76264e4ab26d0837fa67b8b3.png)
应用层传输的数据可能会非常大，直接传输可能就不好控制。就好像老板要求工人搬运物品，如果物品的重量太大超过了工人的运输能力，这时候该怎么办呢，聪明的你马上想到把这一大坨分开运输不就好了？传输层也是同样的逻辑，当数据包的大小超过MSS（TCP最大报文段长度）后就会将其分块处理。这样处理还有一个好处，就是当传输过程中如果有一个分块丢失或者损坏后就只需要单独重传那一个分块就好。在TCP协议中，我们把每一个分块叫做TCP段。
![图源小林！！！](https://i-blog.csdnimg.cn/direct/f945f01f15a34d7d91b47b6e9f54af46.png)

当数据包到达接收方之后又会怎样呢？前面我们说过，传输层是为应用层服务的，那一个设备上那么多应用，我怎么知道这个数据是给谁的？解决这个问题我们只需要对每一个应用编个号就好了，这个编号就是我们的<font color = #00BFFF>端口</font>。

### 1.3 网络层
前面我们提到过，传输层是为应用层服务的，也就是说传输层的工作是为应用层提供端到端的通信服务，确保数据能够准确、可靠且高效地在不同主机的应用程序之间传输。那解决了应用间的通信问题，这些应用又都是运行在设备上的，设备间的通信又该怎么办呢？这时候就该我们网络层出场了。
![图源小林！！！](https://i-blog.csdnimg.cn/direct/fc45d644625641d7820f04b26892622d.png)


在网络层中，我们需要将数据从一个设备传输到另一个设备。同样的，为了实现这样的功能我么也需要遵守一定的协议：IP协议，他会将我们在传输层的数据包继续加装一个IP头部组装成IP报文，同样的如果IP报文数据太大也需要进行分片处理。这时候问题又来了：世界上那么多网络设备，我怎么知道要发给谁？聪明的你马上想到我还是像端口那样，编个号不就得了？但很显然，一个设备上的应用才几个，全世界那么多网络设备编号之后难道还是一个个去匹配？显然不行，我们来看这样一个计算机网络的拓扑图：
![图源网络](https://i-blog.csdnimg.cn/direct/5c51db191e3b4106b08172139d3b7800.png)
可以看到：在计算机网络中，通常会将一些相互连接的设备划分成一个子网，子网内的设备可以直接进行通信。多个子网再通过路由器等网络设备互相连接，最终形成一个更大的网络。聪明的你又马上想到了：我把IP地址分成一个<font color = #00BFFF>网络号</font>，一个<font color = #00BFFF>主机号</font>不就好了？我通过网络号找到设备所在的子网，再通过主机号定位设备不就能极大提升效率！事实也的确如此！（怎么分成网络号和主机号的？只需要通过子网掩码来进行位运算就好了，具体实现推荐大家看小林！）但这时候问题又来了：那么庞大复杂的网络系统，我是把这个地址找到了，但是我一个数据包怎么知道具体该怎么走呢？这就体现到IP协议的路由作用了，不知道？我来告诉你！但他也不想太累，我告诉你怎么走就得了，具体该怎么办，交给我的下一层处理吧！

### 1.4 网络接口层
通过上文我们发现IP协议帮我们找好了怎么从一个子网路由到另一个子网，但问题又来了：子网内部的移动该怎么办呢？这时候就需要引入我们的网络接口层了，在本层中引入了新的协议：MAC，也称为物理地址或硬件地址。他唯一标识了网络设备，通过他，我们终于可以在【链路】这个级别进行数据包的传输了！

### 1.5 小结
综上，我们可以发现整个TCP/IP网络模型就是对数据的一步步封装完善，最终实现其运输功能的一个过程。最后在贴上小林大佬的两张图表示下整个过程：

![图源小林](https://i-blog.csdnimg.cn/direct/7bcc804934d147a5a986be549cf1d555.png)
![图源小林](https://i-blog.csdnimg.cn/direct/4989c948a3ea4dc5acf9750b454ad3b5.png)

## 2.键入网址到网址显示，数据包他经历了什么？
### 2.1 解析地址，生成HTTP请求
我们知道，要想得到一个网址的相关内容信息，我们当然要想他发起请求，那这个HTTP请求需要我们自己写吗？当然不用，我们只需要输入网址就好了，具体的HTTP请求只需要交给我们的第一层：应用层实现就好了。那他是怎么完成从网址到HTTP请求的转变的呢？

让我们来看下我们输入的网址是怎样的：https://www.csdn.net，通过这个网址，我们可以很轻松的导航到CSDN网站的首页，最前面的https：指明访问数据的协议，后面一个“//”说明后面输入的资源是服务器名称，于是通过这样一个网址我们就导航到了CSDN，那我们继续看这样一个网址：https://blog.csdn.net/2303_78315474/article/details/147564841?spm=1001.2014.3001.5501，https://blog.csdn.net到这为止的结构都和前面的类似，那后面跟的又是什么呢？其实这指的是具体要请求的服务器下的文件。

于是我们明白了，键入的地址实际就是指明了要通过某个协议访问某个服务器下的文件资源，于是我们的应用层通过解析这样的请求，便生成相关的get或者post请求并发送。

### 2.2 解析域名，得到真实IP地址
我们知道，当我们需要访问一个服务器时，我们需要定位他的<font color = #00BFFF> IP地址</font >并向这个地址发送请求。但问题来了，我们键入网站的时候输入的不是域名吗？这里其实是通过<font color = #00BFFF> DNS服务器</font >帮我们完成的从域名到IP地址的转化（为什么不直接输入IP地址？因为IP地址太难记了！），那他是怎样实现从域名到IP地址的转化的呢？

现在有一个域名如下：www.example.com，当我们键入他的时候客户端首先会检查本地缓存，如果没有便继续将地址发给本地DNS服务器，如果本地DNS服务器还没有相关缓存，便开始通过<font color = #00BFFF> 递归查询</font >获取结果，过程如下：

1.根 DNS 服务器（Root DNS Server）：本地 DNS 服务器首先向根服务器发送查询请求，根服务器返回负责顶级域名（如.com、.org）的  顶级域名服务器（TLD Server） 的地址。

2.顶级域名服务器（TLD Server）：本地 DNS 服务器再向对应的 TLD 服务器查询，TLD 服务器返回负责该域名的 ** 权威 DNS 服务器（Authoritative DNS Server）** 的地址。

3.权威 DNS 服务器：本地 DNS 服务器最后向权威服务器查询，权威服务器返回该域名对应的 IP 地址。
这里还是用小林大佬的图概括下整个过程：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e9ef4e60db1243bb904b53ae9a2bb0ef.png)

### 2.3 调用协议栈，开始多层封装
现在我们终于知道要请求的服务器的地址了，但是我们该怎样才能委托下层结构帮我们封装这个HTTP请求呢？这时候我们就需要通过系统调用（System Calls）与协议栈进行交互，将网络通信任务委托给协议栈处理。这种交互通常通过套接字（Socket）API实现。协议栈工作的具体流程如下：

1.数据发送流程（应用程序→网络）
步骤 1：应用层生成数据
应用程序（如浏览器、邮件客户端）创建数据，并调用网络 API（如socket.send()）将数据传递给协议栈。

示例：浏览器发送 HTTP 请求：GET /index.html HTTP/1.1。

步骤 2：传输层封装数据（TCP/UDP）
传输层将应用层数据封装为段（Segment）或数据报（Datagram）：

TCP：添加源端口、目标端口、序列号、确认号等，提供可靠传输。
UDP：仅添加源端口和目标端口。（也可以基于UDP实现可靠传输）

步骤 3：网络层封装 IP 数据包
网络层将传输层数据封装为IP 数据包（Packet）：

添加源 IP 地址、目标 IP 地址、协议类型（TCP=6/UDP=17）等。
通过路由表确定下一跳路由器。


步骤 4：网络接口层封装帧
网络接口层（数据链路层 + 物理层）将 IP 数据包封装为帧（Frame）：

添加源 MAC 地址、目标 MAC 地址（通过 ARP 协议解析目标 IP 对应的 MAC）。
添加帧头和帧尾校验（如 CRC）。

步骤 5：物理层传输信号
帧转换为电信号、光信号或无线信号，通过物理介质（网线、光纤、WiFi）传输。


2.数据接收流程（网络→应用程序）
步骤 1：物理层接收信号
网卡接收物理信号，转换为数字比特流，传递给网络接口层。

步骤 2：网络接口层解封装帧
检查帧的完整性（CRC 校验），剥离 MAC 头部，将 IP 数据包传递给网络层：
验证目标 MAC 地址是否为本机。

步骤 3：网络层解封装 IP 数据包
检查 IP 头部，剥离 IP 头部，将数据段传递给传输层：
验证 IP 校验和。
根据目标 IP 判断是否为本机。
根据协议字段（6=TCP/17=UDP）确定上层协议。

步骤 4：传输层解封装段 / 数据报
检查传输层头部，剥离头部，将数据传递给应用层：
TCP：通过序列号重组数据，确认应答。
UDP：直接传递数据（无重组）。
根据端口号（如 80=HTTP）确定目标应用程序。

步骤 5：应用层处理数据
应用程序接收数据并处理：
示例：浏览器解析 HTTP 响应，渲染网页。


3.关键机制
封装与解封装
封装：发送方逐层添加头部（应用层→传输层→网络层→网络接口层）。
解封装：接收方逐层剥离头部（反向顺序）。

分用（Demultiplexing）
传输层通过端口号将数据准确交付给对应的应用程序：
例如：HTTP 请求→80 端口，HTTPS→443 端口。

路由选择
网络层通过路由表确定数据包的下一跳：
本地网络直接交付，远程网络通过路由器转发。

错误控制
TCP：通过确认应答、超时重传、滑动窗口实现可靠传输。
数据链路层：通过 CRC 校验检测帧错误。

以下还是贴下小林大佬的图方便大家阅读：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/221976f9a12f4ec8bce866b65d4b885a.png)
这里面ICMP和ARP是啥？

<font color = #00BFFF>ICMP（Internet 控制报文协议）</font>：用于网络设备间传递错误信息（如目标不可达、超时）和诊断消息（如 Ping 命令），帮助维护网络连通性。
<font color = #00BFFF>ARP（地址解析协议）</font>：将 IP 地址解析为物理 MAC 地址，确保数据帧能正确送达局域网内的目标设备。


现在我们知道整个调用协议栈的流程了，让我们开始对每一步进行分析！

### 2.4 让数据变得可靠：TCP
根据上文协议栈的工作流程我们知道在生成HTTP请求并解析处IP地址后需要经由传输层封装TCP头部，那为什么要封装呢？这不是让报文长度变大了吗？原因很简单：我们需要让传输的数据可靠！那数据包有哪些异常情况呢？请看：
1. 数据包丢失
原因：网络拥塞、路由器缓冲区溢出、物理链路故障等。
表现：数据包未能到达目标主机。
2. 数据包乱序
原因：网络中的多条路径导致不同数据包的传输延迟不同。
表现：数据包到达顺序与发送顺序不一致。
3. 数据包重复
原因：发送方超时重传时，原数据包可能仍在网络中。
表现：同一数据包被多次接收。
4. 数据损坏
原因：物理噪声、电磁干扰等导致比特位翻转。
表现：数据包中的数据发生错误。
5. 网络拥塞
原因：大量数据同时占用有限带宽，导致路由器缓冲区溢出。
表现：延迟增加、丢包率上升。

基于TCP协议，我们可以系统性的解决上述这些异常，实现端到端的可靠传输。现在我们知道了，虽然封装TCP头部确实让报文长度变大了，但为了数据的可靠传输，这是值得的！(TCP具体怎么实现这样的功能的这里先不提及，下文有关TCP协议具体实现时会详细讲述！)

### 2.5 远程定位：IP
现在我们终于有了可以可靠传输的数据包了！但是问题又来了：我是知道服务器的IP地址了，我该怎么从当前的子网路由过去呢？这时候就要基于IP协议来实现了。现在让我们结合IP报文头部来分析下是怎么实现这样远程定位的功能的：
首先来看IP报文头部有哪些字段：

版本号：占 4 位，标识 IP 协议的版本，常见的有 IPv4 和 IPv6。不同版本的 IP 协议在报文格式和功能上有所不同，路由器等网络设备会根据版本号来正确处理 IP 报文。

首部长度：占 4 位，指出 IP 报文头部的长度。由于 IP 头部可能包含可变长度的选项字段，所以需要这个字段来确定头部的实际长度，以便路由器正确解析报文。

区分服务字段：占 8 位，用于指定报文的服务质量要求，如优先级、延迟、带宽等。路由器可以根据这些信息对不同类型的报文进行差异化处理，以满足不同应用的需求。

总长度：占 16 位，标识整个 IP 报文（包括头部和数据部分）的长度。这有助于接收方确定报文的边界，以及在传输过程中检测报文是否完整。

标识符、标志位和片偏移：这三个字段用于处理数据包的分片和重组。当一个 IP 报文长度超过链路层的最大传输单元（MTU）时，需要将其分成多个分片进行传输。标识符唯一标识一个报文，标志位用于指示是否还有更多分片以及当前分片是否是最后一个分片，片偏移则表示该分片在原始报文中的位置，以便接收方正确重组报文。

生存时间（TTL）：占 8 位，用于防止报文在网络中无限循环。每经过一个路由器，TTL 值减 1，当 TTL 值减为 0 时，路由器将丢弃该报文，并向源主机发送 ICMP 超时报文。
协议字段：占 8 位，标识上层协议，如 TCP（6）、UDP（17）等。这使得接收方能够将 IP 报文正确地交付给相应的上层协议进行处理。

首部校验和：占 16 位，用于检测 IP 报文头部在传输过程中是否发生错误。计算方法是对 IP 头部的每个 16 位字进行反码求和，接收方通过重新计算校验和来验证报文的完整性。

<font color = #00BFFF>源 IP 地址和目的 IP 地址</font>：各占 32 位，分别标识发送方和接收方的 IP 地址。

这里面源 IP 地址和目的 IP 地址两个字段是 IP 协议实现定位和路由的关键：路由器根据目的 IP 地址来决定如何将报文转发到目标子网。其他字段主要功能在于维护数据包以及标识长度等。

这里同样借用小林大佬的图来帮助大家理解：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a7d987f4b205423097219f23c312a945.png)


那具体又是如何根据这两个字段来进行路由的？

**子网掩码与 IP 地址匹配**：在 IP 网络中，每个子网都有一个子网掩码。源主机和路由器通过将 IP 地址与子网掩码进行逻辑与运算，得到网络地址。例如，对于一个 C 类网络 192.168.1.0，子网掩码为 255.255.255.0，主机 A 的 IP 地址为 192.168.1.10，通过与运算可以得到网络地址 192.168.1.0，表明主机 A 属于这个子网。路由器在接收到 IP 报文时，会根据目的 IP 地址和自身保存的路由表中的子网掩码信息，判断目标主机所在的子网。

**路由表查询**：路由器维护着一张路由表，其中包含了网络地址、子网掩码、下一跳地址和出接口等信息。当路由器接收到一个 IP 报文时，它会根据目的 IP 地址和子网掩码在路由表中查找匹配的条目。如果找到完全匹配的子网条目，就按照该条目中指定的下一跳地址和出接口将报文转发出去；如果没有找到完全匹配的条目，路由器会使用默认路由（通常是指向连接到其他网络的网关）进行转发，或者根据最长前缀匹配原则选择一个最接近的子网条目进行转发。

**逐跳转发**：报文在网络中从一个路由器转发到另一个路由器，直到到达目标子网。每个路由器都会根据报文的目的 IP 地址和自己的路由表进行转发决策，这个过程称为逐跳转发。在转发过程中，路由器会更新 IP 报文头部的一些字段，如 TTL 值减 1、重新计算首部校验和等。

**目标子网定位**：当报文到达目标子网所在的路由器时，该路由器会根据目标 IP 地址在子网内进行广播或直接将报文发送到目标主机（如果知道目标主机的 MAC 地址）。目标主机接收到报文后，会根据 IP 报文头部的源 IP 地址、协议字段等信息进行相应的处理，并返回响应报文（如果需要）。响应报文按照类似的过程从目标主机返回到源主机。

现在我们终于可以定位并路由到对方所在的子网了，但是子网间的运输怎么办？

### 2.6 两点运输：MAC
为了实现子网间的传输，我们还需要在数据包的头部添加MAC地址，那他是怎样实现这样的功能的？还是先来看MAC报文头部的字段：

目的 MAC 地址：占 6 字节，标识数据帧的目标接收设备的 MAC 地址。这个地址是唯一的，用于在局域网中准确地将数据帧发送到目标设备。

源 MAC 地址：占 6 字节，标识发送数据帧的设备的 MAC 地址。接收方可以通过这个字段知道数据帧的来源。

类型 / 长度字段：占 2 字节。如果该字段的值大于 0x0600，则表示上层协议类型，如 0x0800 表示 IP 协议，0x0806 表示 ARP 协议等，用于指示接收方将数据帧交给哪个上层协议进行处理；如果值小于等于 0x0600，则表示数据字段的长度。

**那目的MAC地址和源MAC地址又是怎么得到的呢？**

源MAC地址：我们知道，MAC地址是用来唯一标识一个物理设备的，因此源MAC地址是在生产时就已经写入，我们只需要将其读出并封装就好了

目的MAC地址：他的获取比较复杂，我们主要讲解使用ARP协议获取目的MAC地址的方法：

**通过 ARP 协议获取**：在局域网中，当设备要向另一台设备发送数据时，它首先会检查自己的 ARP 缓存表。如果缓存表中存在目标 IP 地址对应的 MAC 地址，就直接使用该 MAC 地址作为目的 MAC 地址。如果缓存表中没有相关记录，设备就会发送一个 ARP 请求广播包，询问目标 IP 地址对应的 MAC 地址。局域网中的其他设备收到 ARP 请求后，会检查自己的 IP 地址是否与请求中的目标 IP 地址匹配。如果匹配，该设备就会发送一个 ARP 响应包，将自己的 MAC 地址告知发送方。发送方收到响应后，会将目标 IP 地址和对应的 MAC 地址存入 ARP 缓存表，以便下次使用。

以下还是使用小林大佬的图方便大家理解：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/144c95c9ad8f4276a36ac2909400d80a.png)
**到这我们终于封装好了完整的数据包了，那该怎么发出去呢？**

### 2.7 经由网卡发送
通过上述一系列的操作，我们终于得到了一个数据包，但这只是存储在内存中的一串二进制数字信息，我们还需要把它转化为电信号才能进行传输。这步的操作是由网卡实现的：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/12e0b24143b14ff39e5b3092a9b739c6.png)
通过这张图我们可以发现网卡是在硬件层面的，因此要控制它还需要网卡驱动程序，那数据包是怎么传递的？

**系统调用**：协议栈通过操作系统的网络接口（如socket.send()）将数据帧传递给网卡驱动程序。

**DMA（直接内存访问）**：现代网卡使用 DMA 技术直接从系统内存读取数据帧，避免 CPU 参与数据传输，提高效率。

到了网卡内部又是怎么处理的？

 网卡内部处理流程
 
步骤 1：接收数据帧
网卡驱动将数据帧写入网卡的接收缓冲区（Ring Buffer），并通知网卡有新数据待发送。

步骤 2：数据帧处理
校验与过滤：网卡硬件检查数据帧的完整性（如 CRC 校验），并根据 MAC 地址过滤非目标帧（仅接收目的 MAC 为本机或广播地址的帧）。
解封装：剥离数据帧的 MAC 头部，提取 IP 数据包，进一步传递给上层协议处理。

步骤 3：数据帧发送准备
当网卡需要发送数据时，驱动程序将待发送的数据帧从系统内存复制到网卡的发送缓冲区。
网卡硬件为数据帧添加前导码（Preamble）和帧起始定界符（SFD），用于同步接收方的时钟。

经由这些步骤后网卡就可以将数字信号转为电信号最终发送。

以下还是使用小林大佬的图帮助大家理解：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4a78e4024da041b3b9b7c1d5e968a5fb.png)
### 2.8 交换机转发
前面我么提到过在子网内的传输需要用到MAC地址，那是谁来完成的传输？这里就需要交换机，交换机为什么可以实现这样的功能：是因为交换机拥有多个端口，可将多台设备（如电脑、服务器、打印机等）连接在一起，形成一个局域网。它为设备之间提供了物理上的连接通路，使这些设备能够相互通信。

那他又是怎样实现数据帧的转发的？

交换机根据数据帧的目的 MAC 地址进行转发决策。当交换机收到一个数据帧时，它会在 MAC 地址表中查找目的 MAC 地址对应的端口。如果找到匹配的条目，交换机就会将数据帧从对应的端口转发出去，只将数据发送到目标设备所在的端口，而不是像集线器那样将数据广播到所有端口，这样可以有效地减少网络中的数据流量，提高网络效率。如果在 MAC 地址表中没有找到目的 MAC 地址，交换机通常会将数据帧广播到除接收端口外的所有其他端口，以尝试找到目标设备。当目标设备响应时，交换机就会学习到该设备的 MAC 地址与端口的对应关系，并更新 MAC 地址表，以便后续更准确地转发数据。

也就是说，交换机内部有一张存储MAC地址和端口的表，当解析出数据包中的目标MAC地址后就会查询这张表，有就直接转发。没有的话就广播并等待响应，并且交换机是可以学习的，也就是说响应后会将该记录添加入表中。（我在学习过程中曾有个短暂的疑问：广播后我咋知道这个是发给我的还是别人的？其实很简单：数据包里有目标MAC地址，而每个设备又有自己唯一的MAC地址，我把这俩地址比对以下不就知道了，不是发给我的就忽略好了）

还是用下小林大佬的图：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a123def81f3f4ddd8d2cb612bbbf79e7.png)
### 2.9 开始路由！
终于在子网间完成传送了，下面就需要通过路由器了。路由器的工作其实和交换机是差不多的，都是接受包并查表发送。下面我们来讲下他每一步的具体实现：

**包接受操作**
物理层接收：路由器的物理接口接收来自网络传输介质（如网线、光纤等）的电信号或光信号。这些信号经过物理层的处理，如解码、同步等操作，将其转换为数字信号。

数据链路层解封装：数字信号被传递到数据链路层，数据链路层协议根据相应的帧格式对信号进行解封装，去除帧头和帧尾的相关信息，如以太网帧的前导码、帧起始定界符、目的 MAC 地址、源 MAC 地址、帧校验序列等。通过检查帧校验序列来验证帧的完整性，如果帧校验失败，路由器通常会丢弃该帧。

帧类型检查：数据链路层还会检查帧的类型字段，以确定该帧所承载的上层协议类型，如 IP 协议、ARP 协议等。根据不同的协议类型，将帧中的数据部分传递给相应的上层协议处理模块。对于 IP 数据包，会将其传递到网络层进行进一步处理。

**查询路由表确定输出端口**
提取目的 IP 地址：在网络层，路由器从接收到的 IP 数据包中提取出目的 IP 地址。这个地址是数据包要到达的最终目标设备的 IP 地址。

最长匹配查找：路由器使用目的 IP 地址在路由表中进行查找。路由表是一个存储了网络地址与下一跳信息以及输出端口等对应关系的数据结构。查找过程采用最长匹配原则，即路由器会在路由表中寻找与目的 IP 地址匹配最长的前缀条目。例如，路由表中有一条目的网络为192.168.1.0/24的条目，还有一条192.168.1.128/25的条目，当目的 IP 地址为192.168.1.130时，路由器会选择192.168.1.128/25这条条目，因为它与目的 IP 地址的匹配位数更多。

确定输出端口和下一跳信息：找到匹配的路由表条目后，路由器从该条目中获取输出端口号以及下一跳地址（如果是直接连接的网络，则下一跳地址为目的设备的 IP 地址）。输出端口号指定了数据包应该从路由器的哪个接口发送出去，而下一跳地址则指示了数据包在到达最终目的之前要转发到的下一个路由器或目的设备的地址。

**路由器的发送操作**
网络层处理：在将数据包从输出端口发送出去之前，路由器可能会对数据包进行一些网络层的处理。例如，将 IP 数据包的 TTL（生存时间）字段减 1，以防止数据包在网络中无限循环。如果 TTL 值减为 0，路由器会丢弃该数据包，并向源设备发送一个 ICMP（互联网控制消息协议）超时消息。此外，路由器还可能根据需要对数据包的首部进行其他一些修改，如更新校验和等。

数据链路层封装：路由器将处理后的 IP 数据包传递到数据链路层，数据链路层根据输出端口所连接的网络类型和链路层协议，对数据包进行重新封装。例如，如果输出端口连接的是以太网网络，数据链路层会添加以太网帧头，包括目的 MAC 地址、源 MAC 地址、类型字段等信息。目的 MAC 地址通常是下一跳设备的 MAC 地址，如果下一跳是直接连接的目的设备，则为目的设备的 MAC 地址。源 MAC 地址是路由器输出端口的 MAC 地址。类型字段标识了上层协议为 IP 协议。此外，数据链路层还会添加帧尾，包括帧校验序列等信息，用于检测数据传输过程中的错误。

物理层发送：经过数据链路层封装后的帧被传递到物理层，物理层将其转换为适合在特定传输介质上传输的信号形式，如电信号或光信号，并通过输出端口将信号发送到网络中。信号在传输介质上传播，最终到达下一跳设备或目的设备。

简单来说就是路由器首先接受数据包，并通过检查MAC头部判断是不是发给自己的（路由器是有自己的MAC地址和IP地址的），是的话就先去掉这个MAC头部然后查询路由表判断转发目标，那怎么判断是否到达终点？路由表中会有一个网关列，为空说明已到达，不为空则里面会记录一个IP地址，通过ARP协议利用IP地址查询目标的MAC地址，找到目标MAC地址后在对其重新封装并通过交换机送到下一个路由器。这里也说明了为什么过程中需要对MAC头反复的去掉添加，因为在数据包传输过程中MAC地址是不断改变的（因为MAC地址是用来完成设备间的传输的，而转发过程中两点是不断变化的。）

使用小林大佬的图帮大家理解下路由表的具体构成：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a1c43eb9f3d34bd6970b883c51cec766.png)
这里我根据自己的理解大概画了个图来表示传输过程，希望大家看后能指出不恰当之处！：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/fc100ccdd5a84cf38a6a33267c73fd2e.jpeg)

通过这个图我想表达的意思是：由原设备发送请求后会交给交换机转发到达路由器，路由器会接受并处理这个请求并开始转发到下一个路由器，最终该请求到达与目的IP所在子网相连的路由器，如果该路由器和目的设备相连，则直接由路由器发送，不相连则通过交换机发送（黑色圈圈表示设备，忘了标注了）

### 2.10 解析数据
现在我们终于将数据包发送到服务器了，那服务器该怎么处理这个数据包呢？

**服务器处理数据包**
链路层处理：服务器的网络接口卡从物理线路上接收电信号或光信号，将其转换为数字信号，并根据数据链路层协议（如以太网协议）对数据包进行解封装，检查帧头中的目的 MAC 地址是否与本机 MAC 地址匹配，丢弃不匹配的帧，同时检查帧的校验和以确保数据的完整性。

网络层处理：剥去数据链路层的帧头和帧尾，将数据包交给网络层。网络层根据 IP 协议检查数据包中的目的 IP 地址，确认是本机 IP 地址后，继续处理。同时，检查 IP 数据包的首部字段，如生存时间（TTL）、协议字段等，根据协议字段确定上层使用的协议（如 TCP、UDP）。

传输层处理：根据传输层协议（如 TCP 或 UDP）对数据包进行进一步解封装。如果是 TCP 协议，服务器会检查 TCP 首部中的端口号，找到对应的应用程序进程，并根据 TCP 的确认机制、流量控制和拥塞控制等机制对数据进行处理，确保数据的可靠传输。如果是 UDP 协议，服务器同样根据端口号将数据交付给相应的应用程序，但 UDP 没有 TCP 那样复杂的可靠性机制。

应用层处理：应用程序接收到传输层送来的数据后，根据应用层协议（如 HTTP、FTP 等）对数据进行解析，提取出请求的具体内容，如 HTTP 请求中的 URL、请求方法（GET、POST 等）、请求头字段以及请求体等信息，然后根据这些信息执行相应的操作，如查询数据库、处理业务逻辑等。

**服务器返回数据**

**应用层封装**：服务器应用程序根据请求的处理结果生成响应数据，并按照应用层协议进行封装。例如，对于 HTTP 请求，服务器会构建 HTTP 响应报文，包括状态码、响应头字段和响应体（如 HTML 页面、JSON 数据等）。

**传输层封装**：将应用层的响应数据交给传输层，传输层根据使用的协议（如 TCP 或 UDP）添加传输层首部。如果是 TCP 协议，会为响应数据分配一个源端口号，设置序列号、确认号等字段，并计算校验和。UDP 则相对简单，只添加源端口号、目的端口号和校验和等字段。

**网络层封装**：传输层封装好的数据再交给网络层，网络层添加 IP 首部，包括源 IP 地址（服务器的 IP 地址）和目的 IP 地址（请求设备的 IP 地址），同时设置其他字段如 TTL、协议字段等，形成 IP 数据包。
链路层封装：网络层的 IP 数据包传递到数据链路层，数据链路层根据物理网络的类型（如以太网）添加链路层首部和尾部，包括源 MAC 地址（服务器的 MAC 地址）和目的 MAC 地址（请求设备的 MAC 地址），以及用于差错检测的校验和字段，形成数据帧。

**数据发送**：数据链路层将封装好的数据帧通过网络接口卡发送到物理网络上，沿着与请求数据包相反的路径传输，经过路由器、交换机等网络设备，最终到达请求设备。请求设备按照与服务器类似的过程，从链路层开始逐层解封装，直到应用层获取到服务器返回的数据。

简单来说就是服务器会层层解封数据并检查，无误后将数据再次封装后返回给我们的请求设备

这里使用小林大佬的图帮助大家理解：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b04e898c891f41ce9386511e878e4b4a.png)
### 2.11 小结
讲完这一整部分我个人感觉数据包的传送就是一个遇到问题解决问题的过程：为了实现设备间的传输引入了MAC头部，为了实现不同子网的定位引入了IP头部，为了保证可靠传输引入了TCP头部...总之希望大家学完后能够对数据包的封装和传送有个系统性的了解！也请大家指出本文中不恰当的地方！那下面让我们来看每个协议的具体实现！

## 3. HTTP协议
### 3.1 HTTP协议：理解Web通信的基石
要理解HTTP协议，我们首先要知道什么是HTTP？HTTP全称是：HyperText Transfer Protocol 即：超文本传输协议。下面我们分别从“超文本”，“传输”，“协议”这三个词来初步认识HTTP。

**超文本**（Hypertext）
定义：超文本是一种非线性的文本组织方式，它通过链接将不同的文本内容关联起来，形成网状结构。与传统线性文本（如书籍、文章）不同，超文本允许读者通过点击链接在不同内容之间自由跳转，获取更加丰富和立体的信息。也就是说，超文本最显著的特征就是它可以基于超链接从一个超文本跳跃到另一个超文本。

在 HTTP 中的作用：HTTP 协议最初的设计目标就是为了传输超文本内容，主要是 HTML（超文本标记语言）文件。HTML 通过标签定义超链接，使得网页可以引用其他网页、图片、视频等资源，形成了万维网（Web）的基础。随着发展，HTTP 传输的内容不再局限于 HTML，还包括 JSON、XML 等数据格式，但 “超文本” 依然是 HTTP 核心应用场景的代名词。

**传输**（Transfer）
定义：在计算机网络中，传输指的是数据在不同设备之间的传递过程。HTTP 中的传输涉及客户端（如浏览器）和服务器之间的数据交换，确保数据能够准确、完整地从一方到达另一方。

传输方式：HTTP 采用请求 - 响应（Request-Response）模式。客户端向服务器发送请求（Request），服务器接收到请求后进行处理，并返回响应（Response）给客户端。整个传输过程基于 TCP/IP 协议，确保数据的可靠传输。例如，当用户在浏览器中输入网址并按下回车键时，浏览器会向对应的服务器发送 HTTP 请求，服务器处理请求后返回网页内容，浏览器再将内容解析并显示给用户。

无状态特性：HTTP 是无状态（Stateless）协议，即服务器不保留客户端的任何状态信息。每次请求都是独立的，服务器无法直接知道两次请求是否来自同一个客户端。这一特性简化了服务器的设计，但也带来了一些限制，例如在需要用户登录状态的场景中，需要通过 Cookie、Session 等机制来实现状态管理。

**协议**（Protocol）
定义：协议是一组规则和约定的集合，它定义了通信双方在数据传输过程中必须遵循的格式、顺序和行为规范。通过遵循统一的协议，不同的设备和软件才能实现有效的通信。

HTTP 协议规范：HTTP 协议详细规定了请求和响应的格式、请求方法（如 GET、POST）、状态码（如 200、404）、头部字段（如 Content-Type、User-Agent）等内容。例如，HTTP 请求由请求行（包含请求方法、URL 和协议版本）、请求头（包含各种元信息）和请求体（可选）组成；HTTP 响应由状态行（包含协议版本、状态码和状态描述）、响应头和响应体组成。

版本演进：HTTP 协议有多个版本，每个版本都在功能和性能上进行了改进。常见的版本包括 HTTP/1.0、HTTP/1.1、HTTP/2 和 HTTP/3。例如，HTTP/1.1 引入了持久连接（Persistent Connection）和分块传输（Chunked Transfer）等特性，提高了传输效率；HTTP/2 则通过二进制分帧、多路复用等技术进一步优化了性能；HTTP/3 基于 QUIC 协议，解决了 TCP 协议在某些场景下的性能瓶颈。

现在我们知道了，原来HTTP就是<font color = #00BFFF>在两点之间进行超文本数据传输的一种约定和规范</font>。

前面在将传输时我们提到了HTTP的传输是基于“请求-响应”模式的，下面我们来看一段请求报文和响应保文来更好的理解：

**请求报文**：
```
GET /index.html HTTP/1.1
Host: example.com
User-Agent: Mozilla/5.0...
```
**响应报文**：

```
HTTP/1.1 200 OK
Content-Type: text/html
Content-Length: 1234
<html>..
```
先来看请求报文中的这段代码：

```
GET /index.html HTTP/1.1
```
这里的GET代表客户端的操作类型，常见的方法还包括：
GET：获取资源（如网页、图片）。
POST：提交数据（如表单、文件上传）。
PUT：更新资源（覆盖已有内容）。
DELETE：删除资源。
HEAD：仅获取响应头（不返回响应体）。

那很显然了，指定了请求的方法，下一步不就该指定方法作用的对象了吗？所以  **/index.html** 这一段其实就是指明请求的资源路径；剩下这个**HTTP/1.1**大家一看就知道指定的是请求的协议版本。

再来看请求头（这里为了更好的讲解，我加了一些参数）：

```
Host: example.com
User-Agent: Mozilla/5.0...
Connection：keep-alive
Content-Type: application/json
Accept: text/html
Cookie: session_id=12345
```
首先是Host、User-Agent，Connection，这三是请求头中的通用字段：Host指明了请求的服务器域名或者IP地址，User-Agent是客户端标识，用于帮助服务器识别请求来源。Connection控制连接状态，常见的如keep-alive

接下来是一些内容相关字段，如：
Content-Type：请求体的媒体类型（如application/json、text/plain）。
Content-Length：请求体的字节长度。
Content-Encoding：请求体的编码方式（如gzip）。（后面这里我没写上去）

再然后就是一些缓存控制字段如：
Cache-Control：控制缓存策略（如no-cache、max-age=3600）。
If-Modified-Since：条件请求，仅当资源在指定时间后修改时返回新内容。

最后是一些认证字段如：
Authorization：包含认证信息（如Bearer token）。
Cookie：客户端发送的会话信息（服务器通过Set-Cookie响应头设置）。（缓存和认证暂时还没讲所以前面的请求中我没有加入，后面会提及）

如果该请求是POST请求后面还会有请求体指明携带的数据。讲到这我们在来大概讲一下GET和POST请求这俩，相信做过苍穹外卖的同学们都很清楚GET其实就是对**查询指定服务器里的资源**（要查哪些一般会在URL里给出参数），而POST其实就是**根据请求体对指定服务器里的资源进行修改**（当然也可以用GET请求实现修改数据或者用POST请求查询数据）。

接下来再来看下响应报文：

**状态行：**
```
HTTP/1.1 200 OK
```

同样，这里的HTTP是用来指定具体协议，后面的200其实就是状态码，用来告知本次请求的结果，常见的状态码如下：

2xx：成功（如200 OK、201 Created）。
3xx：重定向（如301 Moved Permanently）。
4xx：客户端错误（如404 Not Found、403 Forbidden）。
5xx：服务器错误（如500 Internal Server Error）。

**响应头：**

```
Content-Type: text/html
Content-Length: 1234
Server: Apache/2.4.41
Set-Cookie: session_id=12345; Path=/
Cache-Control: max-age=3600
```
这里其实和请求头中的字段是很相像的都包括：通用头字段、内容相关字段、缓存控制字段、认证与会话字段等，不过响应头中可能还包括**重定向**字段。那啥是重定向？其实就是当前请求的资源的位置发生变动，这时候就需要我们用新的URL来请求。

**响应体：**

```
<html>
  <head><title>Example</title></head>
  <body>Hello, World!</body>
</html>
```
在这里就会返回请求的资源内容，当然也可能返回错误信息（404等）

这里给大家贴一下请求和响应中常见的字段对比表：
| 分类 | 请求头字段 | 响应头字段 | 说明 |
| --- | --- | --- | --- |
| 标识信息 | User-Agent | Server | 客户端用于标识自身的信息，服务器用于标识自身的信息 |
| 媒体类型 | Content-Type | Content-Type | 分别表示请求体和响应体的数据类型，如`application/json`、`text/html`等 |
| 会话管理 | Cookie | Set-Cookie | 请求时携带的会话相关信息，服务器设置的用于标识会话的信息 |
| 认证相关 | Authorization | WWW-Authenticate | 请求时携带的认证信息，服务器用于指示客户端进行认证的信息 |
| 缓存控制 | Cache-Control | Cache-Control | 用于控制缓存策略，如设置缓存过期时间等 |
| 条件请求相关 | If-Modified-Since | Last-Modified | 请求时用于检查资源是否在指定时间后被修改，响应时表示资源的最后修改时间 | 

读到这里大家可以自然的联想出以下三个问题：
1.我每次都请求相同的文件资源，难道每次都要等待服务器的响应？
2.既然POST请求可以根据请求体中的数据篡改服务器中的文件资源，那有人得到交互信息发送请求恶意篡改数据怎么办？
3.服务器如果不记忆HTTP的状态，岂不是每次请求都要验证身份信息？

所以接下来我们讲解：**HTTP缓存**、**HTTPS加密**以及**Cookie、Session、Token**的相关内容

### 3.2 HTTP缓存
前面我们提到了针对重复的HTTP请求，我们不想反复的从服务器请求数据。于是我们自然想到了将数据以缓存在本地，这样我们就能减少HTTP请求的数量从而提高性能。实现HTTP缓存有两种方式：**强制缓存**和**协商缓存**。

**强制缓存：**
**工作原理**：强制缓存会在浏览器首次请求资源时，将资源存储在本地缓存中，并设置一个过期时间。在过期时间内，浏览器再次请求相同资源时，会直接从本地缓存中获取，而不会向服务器发送请求。

**相关字段**：
1.**Expires**：是 HTTP/1.0 中定义的一个响应头字段，它指定了资源的**绝对过期时间**。例如，Expires: Thu, 15 Apr 2025 20:00:00 GMT表示资源将在 2025 年 4 月 15 日 20 点过期。如果当前时间小于这个过期时间，浏览器就会使用缓存中的资源。

2.**Cache - Control**：是 HTTP/1.1 中定义的通用缓存控制头字段，优先级高于Expires。它有多个取值，常见的如max - age，用于指定资源**可以被缓存的最大时间**（可以认为是一个相对时间），单位是秒。例如，Cache - Control: max - age=3600表示资源在接下来的 3600 秒（1 小时）内有效，在此期间浏览器会直接从缓存中获取资源

一般来说，由于**Cache - Control**的选项更多，设置更加精细，大部分时候会使用他来实现强缓存。大致流程如下：

**一、首次请求资源**
客户端发送请求：浏览器向服务器请求资源（如 HTML、CSS、JS 文件）。
服务器响应：服务器返回资源，并在响应头中设置Cache-Control字段，指定缓存策略。

```
HTTP/1.1 200 OK
Cache-Control: max-age=3600  # 资源缓存1小时（3600秒）
Content-Type: text/javascript
Content-Length: 12345
```
浏览器缓存资源：浏览器将资源存入本地缓存（如内存缓存或磁盘缓存），并记录Cache-Control指定的缓存时间。

**二、再次请求相同资源**
检查缓存有效性：浏览器在发起请求前，先检查本地缓存中是否有该资源，并验证缓存是否过期。

未过期：若当前时间距离上次请求未超过max-age指定的时间（如 1 小时），则直接使用缓存，不向服务器发送请求。

已过期：若缓存已过期，浏览器会向服务器发送新的请求，进入协商缓存流程（可能使用ETag或Last-Modified验证资源是否更新）。

（感觉大部分缓存实现的思路都是这样：没有缓存则读取“数据库”并把数据加载入缓存，有缓存直接读缓存）

**协商缓存：**

**工作原理**
当浏览器发起请求时，首先检查本地缓存中是否有对应的资源。如果有，且资源已过期或者Cache - Control设置为no - cache等需要与服务器协商的情况，浏览器会向服务器发送带有特定验证信息的请求。
服务器接收到请求后，根据验证信息判断资源是否有更新。如果资源未更新，服务器返回304 Not Modified状态码，告知浏览器可以使用本地缓存的资源；如果资源已更新，服务器则返回200 OK状态码，并携带更新后的资源。

**相关字段**
**Last - Modified与If - Modified - Since**
Last - Modified：服务器在响应头中使用该字段，指示资源在服务器上的最后修改时间。例如Last - Modified: Thu, 15 Apr 2025 10:00:00 GMT。

If - Modified - Since：浏览器在后续请求中会将之前响应中Last - Modified的值放在If - Modified - Since字段中发送给服务器。服务器通过比较这个时间与资源实际的最后修改时间来判断资源是否更新。

**ETag与If - None - Match**
ETag：是服务器为资源生成的唯一标识，通常是一个哈希值或版本号，代表资源的特定版本。如ETag: "abc123"。

If - None - Match：浏览器将之前响应中的ETag值放在If - None - Match字段中发送给服务器。服务器通过比较这个值与当前资源的ETag来确定资源是否发生变化。

**协商流程**
首次请求：浏览器向服务器请求资源，服务器正常响应资源，并在响应头中添加Last - Modified和 / 或ETag字段。

再次请求：浏览器检查本地缓存，发现资源已存在但可能过期或需要验证。于是，浏览器在请求头中带上If - Modified - Since（值为上次响应中的Last - Modified）和 / 或If - None - Match（值为上次响应中的ETag）字段发送请求。

服务器验证：服务器收到请求后，根据If - Modified - Since和If - None - Match的值进行验证。如果资源未更新，即If - Modified - Since指定的时间之后资源没有被修改，或者If - None - Match的值与当前资源的ETag匹配，服务器返回304 Not Modified状态码，且不返回资源内容，浏览器使用本地缓存。如果资源已更新，服务器返回200 OK状态码，并携带更新后的资源以及新的Last - Modified和 / 或ETag字段。

**两者对比**
精度：ETag的精度更高，因为Last - Modified只能精确到秒级，如果在一秒内资源多次变化，Last - Modified无法准确识别。而ETag可以根据资源的内容生成唯一标识，能更准确地判断资源是否变化。
性能：Last - Modified只需要比较时间戳，相对性能较好。ETag需要计算资源的标识并进行比较，开销略大。
适用场景：对于经常变动但对**时间精度要求不高**的资源，**Last - Modified**比较合适；对于**内容变化频繁且需要精确控制缓存**的资源，**ETag**更优。

这里我们总结下协商缓存的思想：强制缓存的思想是只要我浏览器判断出数据没有过期，那我直接使用本地缓存。整个过程是**不需要服务器来判断的**。但协商流程核心在于浏览器需要和服务器进行协商，并最终根据**协商结果**来判断是否需要使用本地缓存。这个协商结果怎么来的？ 

第一种实现类似于强制缓存都是通过时间来判断，不过在强制缓存中这个过期时间是在请求完数据后设置的，比较时间也是由**浏览器来和这个设置好的过期时间比较**。而协商缓存的思想是当浏览器请求数据后服务器会返回一个**Last - Modified**字段来指明这个资源在服务器上最后修改的时间，然后在服务器下一次发起请求时还需要带上**If - Modified - Since**这个字段，并由**服务器来比较这两个时间**来判断是否需要更新缓存。如果不需要则返回一个304状态码，这时浏览器就知道可以读取本地缓存了。

第二种实现的思想是基于**“版本号”**的思路来的（个人感觉有点像乐观锁的CAS?）：同样是浏览器请求数据后服务器返回，不过这次返回的**ETag**字段并不和时间相关，它代表的是资源的特定版本（通常是一个哈希值或者版本号），浏览器发现响应头中有这个数据后则会再次像服务器发送请求并将**If - None - Match**的值设置为响应头中ETag的值，最后在由服务器来判断**ETag**是否变化，变化就说明资源已过期。

当然服务器返回的头部中是可以同时有**ETag**和**Last - Modified**字段的，但此时**ETag**的优先级会更高，因为**ETag**的精度是更高的。原因在于Last - Modified只能精确到秒级也就是说在秒内的改变他是无法监测到的，而**ETag**是只要数据更改他就会更新，不是基于时间来的。并且**ETag**能更准确地判断资源是否变化。举个例子：在没有修改文件内容情况下文件的最后修改时间可能也会改变，这会导致客户端重新发起请求。

这里使用小林大佬的图给大家演示协商缓存的大致流程：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b750fc1b697a4e07925b53f99cb9d704.png)


注意：协商缓存的这两个字段都是需要搭配强制缓存来使用的，也就是只有当**强制缓存未命中时才会发起协商缓存**。

最后贴一张小林大佬的图帮助大家理解整个缓存过程的逻辑：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/efc9967a8bba48078aa6425042f48cca.png)


### 3.3 HTTPS加密
前面我们提到过，HTTP的明文传输是有风险的，因此对于敏感数据我们需要通过HTTPS加密传输。下面让我么来看下HTTP明文传输存在的安全问题以及HTTPS是如何解决的。

#### 安全问题：

**窃听风险**：由于 HTTP 传输的数据没有加密，攻击者可以通过网络嗅探等手段，拦截并读取传输中的数据。例如，用户在登录网站时输入的用户名和密码、在电商网站上提交的订单信息、银行转账信息等，都可能被攻击者窃取。

**篡改风险**：攻击者不仅可以读取明文数据，还可以在数据传输过程中对其进行篡改。比如，攻击者可以修改用户的订单信息，将商品数量、价格等进行更改，或者在网页中插入恶意代码，用户访问被篡改的网页时，可能会受到恶意软件的攻击。

**冒充风险**：攻击者可以冒充合法的服务器，向用户发送虚假的信息。例如，攻击者可以搭建一个与真实银行网站相似的假冒网站，诱导用户输入账号和密码等敏感信息，然后窃取这些信息进行非法活动。由于 HTTP 没有有效的身份验证机制，用户很难辨别服务器的真伪。

下面我们基于以上的三个问题来聊一聊HTTPS是如何解决的：

**窃听解决：** 为什么会出现窃听？原因在于由于HTTP是明文传输的，只要截取到了HTTP请求，就可以直接从该请求中得到用户的数据。比如我在初中时常背着父母去网吧打游戏，为了避免父母从聊天消息中直接得知我要去上网，我和同学约定好用“去XX地打球”这一信息来代替上网这个信息，这样即使得到了我的聊天信息他们也无法解密这句话的真实含义。HTTPS实现加密的方法也类似，用特定的算法对信息进行加密，只有知道如何解密才能正确读取信息。这种加密的具体实现就是基于**混合加密**

#### 混合加密
什么是混合加密？其实就是结合了**对称加密**和**非对称加密**。它基于非对称加密来安全的交换密钥，在用此密钥来高效处理数据。

我们来看为什么不使用单纯的对称加密和非对称机密：

对称机密的思想是用同一个密钥对信息进行加密和解密，因为使用的同样的密钥来处理，对称加密的**效率是很高的**，但问题在于如果**密钥被窃取，那么所有使用该密钥加密的数据都将被轻易破解**，也就是说吗密钥传递过程是**有风险的**，是**不够安全的**。而非对称加密的思想是使用公钥私钥进行加密解密，公钥是可以**随意分发**的，但由于运算难度大，它的**效率是不够的**。

因此我们把这两种加密的优点稍加结合：**使用非对称加密传递密钥，再用对称加密来处理明文数据**。这样就可以做到即安全又高效。（怎么传递的会在之后的HTTPS连接详细讲解）

#### 摘要算法、签名
在解决了数据加密这个问题后我么来看下怎么处理数据篡改， HTTP中无法避免数据篡改的原因就在于当服务器接受到请求后并不会判断这个请求是否和原来的一致，也就是说如果有人修改了请求数据服务器是不知情的，就会按照错误的数据进行操作。那我们马上想到，这个问题不也可以基于CAS的思想解决吗？即服务器收到传输内容后首先比对和之前的传输内容是否相同，**只有相同才进行处理**。当然我们不会对整个内容进行比对，那不就太麻烦了。所以就引入了**摘要算法**：通过算法先对原内容进行处理得到一个<font color = "00BFFF">哈希值A</font>,服务器收到内容后在对该内容处理得到一个<font color = "00BFFF">哈希值B</font>。最后通过比对这两个哈希值判断内容是否被修改。

这里放下小林大佬的图：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1eade8b7a40a4a73b3895548a747e776.png)
但此时还有问题：原因在于服务器**只检验了内容是否被篡改，而没有校验内容的来源**。也就是说，如果此时有人恶意截取了原请求和哈希值A并改成了自己的请求和哈希值C然后向服务器发送，这时由于服务器没有校验内容的来源，而且这个中间请求没有被篡改过，最终服务器还是会接受这个请求并处理。也就是说**摘要算法只保证了数据没有被篡改，但没法验证数据的来源**。比如现在我有一张60分的试卷并且由于分数是老师写的且老师签了字，我不能自行把这个试卷篡改成100分骗爸妈了，但由于这些试卷都没有写名字，于是我拿了一张100分的卷子给他们看，最终他们还是会以为这张卷子是我的。

于是我们又要想：如何验证内容的来源呢？

这里就又要用到非对称加密，之前我们提到过非对称加密有公私两把密钥，一个可刚开一个只能自行保管，而且这两个密钥是可以**双向加解密**的，也就是说我可以使用任意一把密钥加密，再用另外一把密钥解密就行，但不同的加密解密方法最终的结果也是不同的：

**公钥加密，私钥解密**：实现 数据加密，用于像特定人安全发送数据。因为只有持有私钥的才能解读出内容

**私钥加密，公钥解密**：实现 数字签名，用于实现数据的**不可抵赖性**及验证数据的来源。因为只要公钥能够正确的解密出内容，就说明了这个消息是来源于持有私钥的人发出的。（就比如现在只有我知道一个物品的所在地，然后我告诉所有人该怎样怎样才能拿到这个物品，最终确实根据我的指示拿到这个物品了，就说明这个消息确实是我说的）

因此我们现在可以基于**私钥加密，公钥解密**的思想来验证内容的来源。不过私钥加密的**对象是内容的哈希值而非内容本身**。最后贴出小林大佬的图帮大家理解：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/54d3124b22e64cddbd57bb160249e572.png)
#### 数字证书
前面我们解决了通信内容加密、防篡改、内容来源的问题，但此时仍然不能完全保证通信过程的安全性。原因在于:**客户端在发起请求时并没有去验证服务器的身份**。比如当同时出现孙悟空和六耳猕猴时，唐僧自身难以验证他俩的真伪，这时就可以通过一些**权威机构**来帮助验证（就像观音、佛祖那样）。 接下来让我们来看下数字证书的实现思路：

1.引入权威机构CA（数字证书认证机构）

2.服务器向证书颁发机构（CA）提交数字证书申请，申请信息通常包括公钥、用户身份信息（如网站域名、组织名称、个人姓名等）以及其他相关的证明材料。这些信息将用于 CA 对申请者身份的验证和证书的签发。

3.CA审核通过后，将申请者的[公钥+信息+签名]打包成一个**数字证书**，再用自己的私钥对数字证书的哈希值（通过摘要算法得到）签名（这是为了保证了证书的真实性和完整性）

4.客户端拿到数字证书，首先对数字证书重新计算得到哈希值来校验**证书的完整性**，随后通过CA事先置入到操作系统或浏览器的公钥解密，验证**证书的真实性**（私钥加密，公钥解密）

5.最后客户端用数字证书中的公钥对内容加密（证书是可信的，那其中的公钥也一定是可信的），服务端用自己的私钥解密，保证**数据的机密性**

最后还是使用小林大佬的图帮助大家理解：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/00896e8bdd354fb5990a2a646cd017ab.png)
### 3.4 HTTPS RSA握手
前面我们知道了HTTPS相对于HTTP来说是更安全的，因为他实现了对内容的加密检验等。那他是怎么做到的呢？其实是在HTTP层和TCP层中在添加一层TLS协议，基于它来实现安全连接。即在建立通信之后，再利用TLS握手来保证通信的安全。TLS握手实现密钥交换又是通过RSA或ECDHE来实现的，因此我们这里先讲RSA

还是先用小林大佬的图帮助大家直观感受下：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5e61a3374de54689a639a9fa33cbaa69.png)
（之前我在学习的时候曾短暂的想过为什么TCP在TLS下层但却要先TCP握手后TLS握手，其实这个很简单：TLS握手是来保证HTTP通信的安全的，那要进行HTTP通信则要先TCP握手连接）

#### RSA握手解析
前面我们提到了混合加密的核心是：**非对称加密交换密钥，对称加密处理通信内容**。下面让我们来看下这个过程是怎么通过RSA握手实现的，我把RSA握手大致分成了以下三个阶段：
1.第一阶段：交换信息，确认版本号和密码套件
2.第二阶段：验证服务器的证书，并交换密钥
3.第三阶段：密钥交换完毕，通知并开始加密

现在让我们结合具体四次握手过程来分析如何实现这三个阶段的：

**TLS第一次握手**
客户端向服务器发送一个 ClientHello 消息，其中包含以下信息：
1.客户端支持的 **TLS 版本**。
2.**一个随机数**（Client Random），用于后续生成密钥。
3.客户端**支持的加密算法列表**，包括 RSA 相关的加密套件。
其他一些可选参数，如压缩方法等。

这一步的目的在于告知服务端我支持哪些TLS版本和密码套件，你只能从这里面选择，否则我们可无法通信啊！ （随机数是为了后面对称加密密钥使用的）服务端收到消息，于是回复：

**TLS第二次握手**
**ServerHello**：服务器收到 ClientHello 后，会发送一个 ServerHello 消息作为响应。这个消息包含服务器选择的 TLS 版本、一个随机数（Server Random），以及从客户端提供的加密算法列表中选择的一个具体的加密套件（通常是包含 RSA 算法的套件）。

<font color = "00BFFF"> 到这一步可以认为我自己定义的第一阶段结束了，即客户端服务端确认了版本信息和密码套件</font>

**证书发送（Certificate）**：服务器接着会发送自己的数字证书，这个证书包含了服务器的公钥以及相关的身份信息，并且是由证书颁发机构（CA）使用其私钥签名的。客户端可以通过验证证书的签名来确认服务器身份的真实性。如果服务器的证书是由中间 CA 颁发的，那么服务器还需要发送整个证书链，包括中间 CA 证书和根 CA 证书，以便客户端能够构建完整的**证书信任链**进行验证。

这里我们介绍下**证书信任链**：我们可以先假设一下有洁癖的人是如何清理环境的，即一定相信**某个物品肯定是干净**的，并基于此干净的物品去擦拭处理别的物品。证书信任链也是一样：即**用户一定会相信根证书和预载了根证书的操作系统和浏览器**。那如果当我们遇到申请的证书是由中间证书签发的（假设为A）就会出现这样的场景：由于不确定A是否可信，就去找签发A的证书（假设为B）；B也不知道是否可信，就去找签发B的证书（假设为C）；此时发现C就是根证书，C一定可信，于是用C的公钥验证B，发现B可信；再用B的公钥验证A，最后发现A可信。

一下使用小林大佬的图帮大家直观感受此流程：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6f0667f7df5c400eb5f199b5edc83fc2.png)
为什么需要中间证书？我们不难发现，由于我们信任的基础是根证书，所以就必须保证他的安全，就可以使用多个中间证书讲根证书隔离。

**TLS第三次握手**
**客户端密钥交换**（Client Key Exchange）：客户端收到服务器的证书后，会验证证书的有效性。如果证书有效，客户端会生成一个预主密钥（Pre - Master Secret），并用服务器证书中的公钥对其进行加密，然后将加密后的预主密钥发送给服务器。由于只有服务器拥有对应的私钥，所以只有服务器能够解密得到预主密钥。

<font color = "00BFFF">到这一步就可以认为是我自己定义的第二阶段完成了，即完成交换密钥。剩下就只需要各自通知开始加密并验证信息是否被篡改就好了</font>

**更改密码规范**（Change Cipher Spec）：客户端发送完加密的预主密钥后，会立即发送一个 Change Cipher Spec 消息，通知服务器后续的消息将使用新协商的密钥和加密算法进行加密。

**TLS第四次握手**
**更改密码规范**（Change Cipher Spec）：服务器收到客户端的加密预主密钥后，使用自己的私钥解密得到预主密钥。然后，服务器根据之前交换的两个随机数（Client Random 和 Server Random）以及预主密钥，生成会话密钥（Session Key），用于后续的加密通信。服务器也会发送一个 Change Cipher Spec 消息，告知客户端自己已经准备好使用新的密钥和加密算法进行通信。

**完成消息**（Finished）：服务器发送完 Change Cipher Spec 消息后，会发送一个 Finished 消息。这个消息是对之前所有握手消息的一个摘要，使用新生成的会话密钥进行加密。客户端收到 Finished 消息后，会使用相同的方法生成摘要，并与解密得到的服务器发送的摘要进行对比。如果两者一致，说明握手过程成功，双方可以开始使用协商好的密钥和加密算法进行安全的数据传输。

最后我们再来谈一谈RSA算法的缺陷：它不支持**前向加密**

什么是前向加密：前向加密是一种安全特性，确保即使长期**私钥（如服务器私钥）被泄露**，**过去的通信内容仍保持机密性**。其核心原理是：**每次会话的加密密钥独立生成且不依赖于长期私钥**，即使长期私钥被破解，攻击者也无法回溯解密历史会话数据。通过这段话我们会发现，在TLS第三次握手时客户端发送的预主密钥都是由服务器的私钥解密的，所以只要当服务器的私钥泄露，则可以根据其破解得到过去所有的密文。

因此我们可以发现要想实现前向加密，核心就在于不能依赖长期的私钥，而是每次会话都生成**临时的密钥对**，由此便引出了<font color = "00BFFF">ECDHE算法</font>

#### ECDHE握手解析
这里我们先来讲一下ECDHE背后的数学实现：**离散对数**（由于我们的重点在计网，因此相关数学实现只做简单描述）

我们来看一下这个式子  ： **a^k (mod p) = b** ,这里的a是底数，b是真数，p是模数，k是对数。其中k也称为：**b的以a为底的模p的离散对数**。

现在已知a和p是公开的，那如果**知道k很明显能够算出b**，但如果**知道b想要反推出k在计算上则是不可行的**（没有已知的多项时时间算法）

那么接下来让我们来看下**DH算法**具体是如何实现密钥交换的：

1.基于离散对数知识我们知道首先需要**申明公开的模数（设为p）和公开的底数（设为a）**

2.假设会话双方分别为甲和乙： 甲随机生成一个数x作为私钥、乙随机生成一个数y作为私钥

3.甲用公开值进行如下运算： a^x (mod p) = A,并将A发给乙；同理乙进行如下运算：a ^y (mod p) = B;并将B发给甲

4.现在甲有以下数据：a,p,A,B;乙有以下数据：a,p,A,B。 甲进行如下运算：B^x （mod p）= (a ^y) ^x (mod p) = a ^ xy (mod p) = K; 乙也进行如下运算： A ^ y (mod p) = (a ^ x) ^ y (mod p) = a ^ xy (mod p ) = K;

5.最后将运算得到的K值作为公钥。由于A,B的传输是公开的，**攻击者可以截取得到A,B。但根据离散对数的知识我们知道：想利用a^x (mod p) = A或a ^y (mod p) = B 这两个式子从A或B得到私钥x,y是不可行的**。既然无法得到x,y的值，那想要得到公钥K的值也是不可行的。

现在我们终于可以开始进行**ECDHE算法**的讲解了：
通过上述过程我们不难发现DH算法需要很多乘法运算，效率是不够高的（哪怕使用快速幂还是需要多次计算）。于是基于DH算法的思想我们引申出了ECDHE算法。两者的核心私立都是一样的，下面我们来看下ECDHE的具体实现：

1.还是申明公开参数，不过申明的是选择的**椭圆曲线类型**和**基点G**

2.仍然是双方各自生成一个随机数x（分别记为x1,x2）

3.仍然是基于公开值和随机数进行运算，不过运算过程如下：x1 * G = Q1; x2 * G = Q2。然后还是熟悉的双方互相交换Q1,Q2的值

4.还是利用交换的公开值Q和自己的私钥运算，过程如下：Q2 *  x1 = x2 * G * x1 = K(这样计算会得到椭圆曲线上一点的x坐标)。另一方也做如下运算：Q1 * x2 = x1 * G * x2 = K。最终K就是我们想要的公钥！

基于ECDHE的握手流程和RSA的是很像的（本来思路都是类似的）不过在密钥交换过程中实现还是有所不同，所以我还是把他分成了以下三个阶段：
1.第一阶段：交换信息，确认版本号和密码套件
2.第二阶段：验证服务器的证书，并交换密钥
3.第三阶段：密钥交换完毕，通知并开始加密

下面让我们来看下具体握手过程：

**TLS第一次握手**
和RSA的流程一样，都是告知服务器TLS版本号和密码套件并且发送一个**随机数A**（这个随机数不是作为私钥，而是最后和共享密钥一起生成会话密钥）

**TLS第二次握手**
这里服务器还是会先回复TLS版本号，**随机数B**，密码套件。不过在这里回复的密码套件中会说明使用的是ECDHE算法。随后还是需要发送自己的证书。<font color = "00bfff">接下来由于使用的是ECDHE算法，所以需要指明公开的参数，于是服务端在发送一个[Server Key Exange]信息，这个过程中包含了：**1.选择椭圆曲线和基点公开给客户端  2.生成一个随机数作为私钥并保存到本地
3.根据私钥和基点计算出公钥并公开给客户端**</font> 最后过程完毕，发送一个[Server Hello Done]消息告知客户端（为了保证公钥内容不被篡改，这里还会使用RSA签名算法对公钥处理）

**TLS第三次握手**
这里客户端还是和之前一样先校验证书，证书处理完毕后客户端会**生成私钥，通过公开参数计算公钥并发送给服务端**。这时就会进入我们之前讲的：服务端和客户端会根据自己的参数计算出一个点坐标，且这个点的**x**值是相同的。最后再用第一次握手的**随机数A**+第二次握手的**随机数B** + **x** 三者组合最终得到会话密钥。最后在发送一个Change Cipher Spec信息告知后续开始使用对称加密算法通信

**TLS第四次握手**
和RSA的基本一样还是回复：Change Cipher Spec和Encrypted Handshake Message信息告知使用加密算法并做摘要

### 3.5 会话状态保持
前面我们提到过HTTP是**无状态的**，其实就是说HTTP 协议本身不会在不同的请求之间记住或保留关于客户端的任何信息。这样的特点可以使得服务器不需要额外的资源来记录状态信息，这样就可以减轻服务器的负担。但同时也导致了当进行一连串关联性的操作时会变得很麻烦：比如说用户登录页面，修改信息，查看个人私密数据等，这些操作每次都需要提交用户名和密码。那有没有什么方法可以跳过这些繁琐的步骤呢？ 这里我们主要讲解：**Cookie，Session，Token**

#### Cookie
Cookie的核心思路就是既然操作时每次提要提交信息，那为什么不把这些信息保存，并在之后每次请求时自动带上给服务器验证呢？于是Cookie的工作流程可以这样描述：

**创建 Cookie**：当用户访问一个网站时，服务器会根据用户的操作或网站的逻辑决定是否创建 Cookie。例如，用户登录网站时，服务器可能会创建一个包含用户登录信息（如用户名、用户 ID、登录时间等）的 Cookie，也可能会为了记录用户在网站上的一些偏好设置（如字体大小、语言选择等）而创建 Cookie。服务器通过在 HTTP 响应头中添加Set - Cookie字段来将 Cookie 发送给客户端。

**发送 Cookie 到客户端**：客户端（通常是浏览器）接收到服务器返回的带有Set - Cookie头的响应后，会根据 Cookie 的属性（如域名、路径、过期时间等）将其存储在本地。不同浏览器存储 Cookie 的方式和位置有所不同，但一般来说，它们都会将 Cookie 与相应的域名关联起来，以便后续访问该域名下的页面时能够正确地发送 Cookie。

**客户端发送 Cookie 到服务器**：当客户端再次访问同一网站的相关页面时，会自动在 HTTP 请求头中添加Cookie字段，将存储在本地的与该网站相关的 Cookie 发送给服务器。这样服务器就可以通过读取这些 Cookie 来识别用户身份、获取用户的相关信息或根据用户的偏好来定制响应内容。

**服务器接收和使用 Cookie**：服务器接收到客户端发送的 Cookie 后，会解析Cookie字段，提取出其中的信息。然后根据这些信息来执行相应的操作，如验证用户身份、查询用户的个性化设置、记录用户的访问行为等。服务器可以根据 Cookie 中的信息来生成动态的网页内容，为用户提供个性化的服务。如果服务器需要更新 Cookie 的内容，例如用户修改了密码，服务器会再次通过Set - Cookie头来发送新的 Cookie 值，客户端会用新的 Cookie 替换旧的 Cookie。

可以简单理解为：根据具体情况，将一些数据设置在Cookie里并**保存在浏览器**内，在以后的请求中自动携带Cookie以便服务器接收并解析

#### Session
Session的核心思想就是有些信息我并不想通过Cookie保存在浏览器，于是我接著服务器来保存，后续每次请求我只需带上一个SessionID（用它就可以标识客户端，类似于客户端的身份证），然后服务器根据这个SessionID来识别并获取客户端信息就好了，于是他的工作流程如下：

**客户端首次请求**：当客户端（通常是浏览器）首次访问服务器上的 Web 应用程序时，它会向服务器发送一个 HTTP 请求。此时，客户端没有携带任何 Session 相关的信息。

**服务器创建 Session**：服务器接收到客户端的请求后，会判断该请求是否需要创建一个新的 Session。通常在用户登录、访问需要身份验证的页面或执行某些特定操作时，服务器会为客户端创建一个唯一的 Session。服务器会在内存或其他存储介质（如数据库）中为该 Session 分配空间，并生成一个唯一的 Session ID。这个 Session ID 就像是客户端在服务器上的 “身份证”，用于标识该客户端的 Session。

**服务器返回 Session 信息**：服务器将创建好的 Session ID 通过 HTTP 响应发送给客户端。一般情况下，服务器会将 Session ID 存储在 Cookie 中发送给客户端，这样客户端在后续的请求中就可以通过 Cookie 将 Session ID 携带给服务器。当然，也可以通过 URL 重写等其他方式将 Session ID 传递给客户端，但使用 Cookie 是最常见的方式。

**客户端后续请求**：客户端在收到服务器返回的 Session ID 后，会在后续的每个 HTTP 请求中通过 Cookie 将 Session ID 发送给服务器。这样，服务器就能够根据接收到的 Session ID 来识别客户端，并找到对应的 Session 对象，从而获取该客户端的相关信息和状态。

**服务器验证和使用 Session**：服务器接收到客户端发送的 Session ID 后，会验证该 Session ID 的有效性。如果 Session ID 有效，服务器会从存储中获取对应的 Session 对象，并根据其中存储的信息来处理客户端的请求。例如，服务器可以根据 Session 中存储的用户登录信息来验证用户的身份，根据用户的购物车信息来生成购物车页面等。服务器可以在 Session 中存储各种与客户端相关的数据，如用户的登录状态、用户的偏好设置、购物车中的商品列表等，以便在不同的请求之间共享这些信息。

**Session 更新和销毁**：在客户端与服务器的交互过程中，服务器可能会根据客户端的操作更新 Session 中的信息。例如，当用户在购物车中添加或删除商品时，服务器会更新 Session 中存储的购物车信息。当 Session 达到一定的空闲时间或用户主动注销登录等情况下，服务器会销毁 Session，释放相关的资源。此时，客户端存储的 Session ID 将失效，客户端再次发送请求时，服务器会认为是一个新的客户端请求，可能会重新创建一个新的 Session。

简单来说就是不再让浏览器记录信息了，而是让**服务器记录相关信息**，并**通过Cookie将SessionID返回给客户端**，之后客户端只需要每次带上这个ID，服务器就可以基于它去查找客户端的信息。

#### Token
前面我们提到了Session是保存在服务器里的，但当请求过多时服务器会记录大量信息并且在一些分布式系统里一个请求可能会被发到另一个服务器上，但这个服务器可能没有相关信息于是又只能重新发送请求。那TOKEN是怎么解决这个问题的？他的思路就是服务器不在存储信息，而是存储一个密钥。当用户验证成功后服务器会生成一个JWT并返回。这个JWT中带有签名信息，于是后续请求中客户端都带上这个JWT，服务器只需要根据生成JWT时的密钥和算法重新计算签名就可判断其有效性，最后根据JWT中携带的数据判断是否允许客户端访问，于是流程如下：

**用户登录**：用户在客户端（如网页或移动应用）输入用户名和密码等凭证进行登录。客户端将这些登录信息发送到服务器。
服务器验证用户：服务器接收到登录请求后，对用户提供的凭证进行验证。验证方式通常是查询数据库或其他身份验证机制，以确定用户是否合法。

**生成 JWT**：如果用户验证成功，服务器会创建一个 JWT。JWT 包含三部分，通常用点号（.）分隔：
**头部（Header）**：一般包含两部分信息，即令牌的类型（如 JWT）和使用的加密算法（如 HMAC SHA256 或 RSA）。例如：{"typ":"JWT","alg":"HS256"}。
**载荷（Payload）**：包含用户相关的声明（Claims），例如用户 ID、用户名、角色、权限等信息，也可以包含一些其他的元数据，如过期时间、颁发时间等。这些声明是 JWT 携带的主要信息，例如：{"sub":"1234567890","name":"John Doe","admin":true,"exp":1677721200}，其中exp表示过期时间。
**签名（Signature**）：服务器使用一个只有它自己知道的密钥，结合头部和载荷，通过指定的加密算法生成签名。这个签名用于验证 JWT 的完整性和真实性，确保令牌在传输过程中没有被篡改。例如，使用 HMAC SHA256 算法，签名的计算方式是HMACSHA256(base64UrlEncode(header)+"."+base64UrlEncode(payload),secretKey)。

**返回 JWT**：服务器将生成的 JWT 作为响应发送给客户端。客户端通常会将 JWT 存储在本地，如浏览器的localStorage、sessionStorage或Cookie中，具体存储位置取决于应用的需求和安全策略。
客户端发起请求：在后续的请求中，客户端会将 JWT 包含在请求头中，通常是Authorization头，格式为Bearer <token>。例如：Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWUsImV4cCI6MTY3NzcyMTIwMH0.9BvR280TjB0lK3W45y95g46Wd4c8m9c3Z0J8d0。这样，客户端每次向服务器发送请求时，都能通过 JWT 来证明自己的身份和权限。

**服务器验证 JWT**：服务器接收到带有 JWT 的请求后，会从请求头中提取 JWT，并使用与生成签名时相同的密钥和算法来验证签名的有效性。同时，服务器还会检查 JWT 中的声明，如过期时间、用户权限等，以确定是否允许客户端访问请求的资源。

**返回响应**：如果 JWT 验证成功，服务器会根据用户的权限处理客户端的请求，并返回相应的响应。如果 JWT 验证失败，例如签名无效、令牌已过期或用户没有足够的权限，服务器会返回相应的错误信息，如401 Unauthorized或403 Forbidden。

### 3.6 HTTP升级
下面先让我们来看下HTTP/1.1的不足之处：
**1.1 线头阻塞（Head-of-Line Blocking）**
问题：HTTP/1.1 采用 “请求 - 响应” 的串行模式，同一 TCP 连接上的请求必须按顺序处理。若前一个请求处理缓慢（如等待服务器响应），后续请求会被阻塞，即使它们彼此独立。
影响：页面加载时，若关键资源（如 CSS、JS）的请求被阻塞，会导致整个页面渲染延迟。

**1.2 低效的连接管理**
问题：为绕过线头阻塞，浏览器通常会建立多个 TCP 连接（Chrome 默认为 6 个），但过多连接会增加服务器负担和网络拥塞。
影响：每个连接都有握手、SSL 协商等开销，且多个连接可能竞争带宽，降低整体效率。

**1.3 头部冗余**
问题：HTTP/1.1 请求头通常包含大量重复信息（如 Cookie、User-Agent），且以明文传输，导致每次请求都有额外开销。
影响：在移动网络等带宽受限环境中，头部数据可能占比过高，浪费资源。

**1.4 无优先级机制**
问题：HTTP/1.1 无法为不同资源指定加载优先级，关键资源（如首屏 CSS）可能被非关键资源（如图片）抢占带宽。
影响：页面渲染延迟，用户体验下降。

**1.5 服务器不能主动推送**

那HTTP/2用了哪些技术来处理这些问题：

**2.1 二进制分帧（Binary Framing）**
改进：
HTTP/2 将所有数据（请求、响应）分割为二进制帧（Frame），并使用流（Stream）进行管理。每个帧包含流标识符，使得不同请求 / 响应的帧可以交错传输，最终在接收端重新组装。
实现：
帧（Frame）：最小数据单位，包含头部和负载，如 HEADERS 帧用于传输请求头，DATA 帧用于传输请求体。
流（Stream）：一个双向字节流，多个流可以复用同一个 TCP 连接。每个流有唯一标识符（如 :stream_id）。
二进制协议：相比 HTTP/1.1 的文本协议，二进制解析更高效、更健壮。

**2.2 多路复用（Multiplexing）**
改进：
通过二进制分帧，HTTP/2 允许在单个 TCP 连接上同时处理多个请求和响应，彻底解决线头阻塞问题。客户端和服务器可并行发送和接收帧，无需等待前一个请求完成。
实现：
客户端和服务器将 HTTP 消息拆分为多个独立帧，按优先级交错发送。
接收方根据帧的流标识符重组消息，恢复原始请求 / 响应。
示例：浏览器可在同一连接上并行请求 CSS、JS、图片等资源，互不干扰。

**2.3 头部压缩（HPACK）**
改进：
HTTP/2 使用 HPACK 算法压缩请求头，减少冗余数据。它维护一个静态和动态的字典，对重复的头部字段（如 Cookie）只传输索引值，新值则进行压缩编码。
实现：
静态表：预定义常见头部字段（如 :method, :scheme）及其索引。
动态表：在通信过程中动态学习和存储出现过的头部字段。
霍夫曼编码：进一步压缩头部值，减少传输体积。
示例：多次请求的相同 Cookie 只需在首次传输完整值，后续请求仅传输索引。

**2.4 请求优先级（Prioritization）**
改进：
HTTP/2 允许为每个流分配权重（1-256）和依赖关系，服务器可据此决定资源加载顺序。例如，首屏 CSS 可设为高优先级，图片设为低优先级。
实现：
权重（Weight）：数值越大优先级越高。
依赖关系（Dependency）：指定流依赖于其他流，形成优先级树。
服务器可根据优先级调度资源，确保关键内容先加载。

**2.5 服务器推送（Server Push）**
改进：
服务器可主动向客户端推送资源，无需客户端显式请求。例如，当客户端请求 HTML 时，服务器可同时推送相关 CSS、JS，减少往返延迟。
实现：
服务器发送 PUSH_PROMISE 帧，告知客户端即将推送的资源及对应的流 ID。
客户端可选择拒绝推送（通过 RST_STREAM 帧）。
推送资源缓存后，客户端后续请求可直接使用缓存，提升性能。

**2.6 流量控制（Flow Control）**
改进：
HTTP/2 为每个流和整个连接提供独立的流量控制，防止发送方过度发送数据导致接收方缓冲区溢出。
实现：
接收方通过 WINDOW_UPDATE 帧告知发送方自己的接收窗口大小（可用缓冲区）。
发送方根据窗口大小调整发送速率，实现流量平衡。

下面让我们来看下每个改进方法的具体实现：

#### 头部压缩
首先我们知道HTTP/1.1中存在这样的问题：**1.有很多固定字段 2.请求响应中有很多重复字段**。因此聪明的你马上想到了对应的解决方法：即对于固定的字段使用一张表来存储（静态表），重复的字段使用另一张表动态存储（动态表）。

这里我使用小林大佬的图来表示下：

**静态表**
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b70cc9b6d8654ab998aef60d391e3452.png)
这里的Header Name代表字段名，Header Value代表字段值。其中有些索引没有Header Value值，是因为某些字段的值会因具体请求和响应而异，或者说在一些情况下不需要指定特定值。对于这些字段他们的value值就需要经过**Huffman编码**。Huffman编码的目标就是为一些常用的字符串分配更短的二进制编码。

对Huffman编码的具体实现可以举个例子：

假设我们有一个需要压缩的 HTTP 头部字段值的集合，例如{"Content-Type": "text/html", "Accept-Language": "en-US", "Connection": "keep - alive"}。

首先，统计每个字符出现的频率。在这个例子中，字符"t"出现了 3 次，"e"出现了 4 次，"x"出现了 1 次，"h"出现了 2 次，"m"出现了 1 次，"l"出现了 2 次，"a"出现了 3 次，"n"出现了 2 次，"g"出现了 1 次，"-"出现了 3 次，" "（空格）出现了 2 次，"k"出现了 1 次，"p"出现了 1 次。

然后，根据这些频率构建霍夫曼树。将出现频率最低的字符作为叶子节点，逐步向上构建树。例如，"x"、"m"、"g"等出现频率较低的字符会在树的较低层，而"e"、"t"、"a"等出现频率较高的字符会更靠近根节点。

构建好霍夫曼树后，为每个字符分配编码。从根节点到叶子节点的路径，向左分支记为0，向右分支记为1。例如，假设"e"的编码为00，"t"的编码为01，"a"的编码为10等。

对于"Content - Type: text/html"这个头部字段值，经过霍夫曼编码后可能就变成了类似011011001110...这样的二进制序列，相比原始的字符序列，大大减少了存储空间。

**动态表**
根据刚刚的分析我们知道，静态表只维护了61种高频出现的字符串，那不在这个表里的字符串怎么办？就可以自行创建动态表，里面记录了请求响应中重复出现的字段。这样我们在之后的请求中只需要去查询动态表就可以大大压缩头部长度了

#### 二进制分帧
我们知道在HTTP/1.1中响应的内容是以文本格式呈现，这确实提升了其可读性，但也降低了HTTP的传输效率。于是在HTTP/2中就使用了二进制帧：
二进制分帧的核心是将请求 / 响应分解为 HEADERS 帧 和 DATA 帧（及其他控制帧），其中：

**HEADERS 帧：**通过 HPACK 算法 压缩头部数据（利用静态表 / 动态表索引重复字段，并对字面量值进行霍夫曼编码），帧头包含流 ID 等元数据，负载为压缩后的头部二进制序列。

**DATA 帧**：直接携带消息体的原始二进制数据（如 UTF-8 编码的文本或图片字节流），通过流 ID 与 HEADERS 帧关联，支持分段传输（多个 DATA 帧拼接成完整数据）。

所有帧通过 流 ID 标识所属的请求 / 响应，在同一个 TCP 连接中交错传输，实现多路复用；接收端根据帧头信息重组数据，解压缩头部并还原原始内容。”


如下图：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/54a5ea7b32d0460f840a74ebe5bdbd76.png)
#### 并发传输
接下来让我们来看下HTTP/2是怎么在二进制帧的基础上实现的并发传输。在HTTP/1.1中，为了实现并发传输采用多TCP连接，但每次TCP连接前都需要三次握手并且TCP是慢启动的，所以这种方式的效率并不是很高。既然多TCP连接不行，那为什么不直接在一条TCP连接里传递多次的请求/响应实现**多路复用**呢？于是可得到HTTP/2的实现思路：

1.建立连接

2.在一个TCP连接中为**每个请求创建流**（Stream），这个流是全双工的，也就是服务器和客户端都可以往流里发送消息，每个消息就是一个**Message**，Message里的信息就是已经被压缩处理后的**二进制帧**（帧里还可以设置优先级，服务器根据优先级选择响应顺序）

3.服务器优先响应高优先级的流，并通过Data帧往流里发送数据。同时也**并行处理**其他流

4.客户端接收：客户端根据流ID **重组数据**

我们可以发现：**数据可重组**、**可设置优先级**。这些特性又是怎么实现的呢？

这里借用小林大佬的图：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/cbe1d1f5aa6c405d8b3a877b281e1fcf.png)
这里我们重点看：**帧类型、标志位、流标识符**。 其中帧类型可分为两大类：**数据帧和控制帧**（控制帧就是实现各种控制信息）；数据帧除开之前提到过的Header和Data还有一个**Priority**，这个就是用来**控制流优先级**的。设置类型后接下来的标志位就会**携带一些简单的控制信息**（比如优先级的值、数据发送结束标志等）。 最后的**流标识符**就是用来记录每个流的ID的，这样我们就可以实现在同一个TCP连接里**乱序发送流**（但流里的数据是需要顺序处理的），因为最终都可以根据ID重新组装（这样也解决了队头阻塞问题，因为就算当前流阻塞也不会影响别的流）

最后我们来总结下HTTP/2并发传输的相关特性：

| **特性**   | **说明**                                                                 |
|------------|--------------------------------------------------------------------------|
| 独立性     | 不同流的帧可在同一TCP连接中并行传输，互不干扰。                          |
| 有序性     | 同一流内的帧必须按顺序处理，确保请求/响应的语义正确。                    |
| 双向性     | 客户端和服务器均可在流上发送帧，支持全双工通信。                          |
| 可控制性   | 可通过PRIORITY帧调整流的优先级，或通过RST_STREAM帧取消流。               |

#### 主动推送
在HTTP/1.1中是没有实现服务器主动推送的，也就是说任何想要的数据都需要基于请求/响应才能得到。于是在HTTP/2中实现了服务器主动推送。下面让我们来看下其原理和实现：

**1. 核心原理**
服务器通过分析客户端的初始请求（如 HTML 页面），预测客户端后续可能需要的资源（如 CSS、JS、图片），并主动将这些资源推送到客户端缓存。客户端在后续需要这些资源时，可直接从本地缓存读取，无需再次发送请求。

**2. 实现机制**
2.1 PUSH_PROMISE 帧
服务器使用 PUSH_PROMISE 帧 通知客户端即将推送的资源：

**包含内容：**
被推送资源的请求头（如 :path、Content-Type）。
新分配的流 ID（用于标识推送的资源，必须为偶数）。

**流程：**
服务器收到客户端对 index.html 的请求（流 ID 1）。
服务器预测客户端需要 style.css 和 main.js，发送两个 PUSH_PROMISE 帧，分别分配流 ID 2 和 4。
客户端接收后，可选择接受或拒绝推送（通过 RST_STREAM 帧）。

**2.2 推送资源的传输**
服务器通过新的流（如流 ID 2 和 4）发送被推送资源的 HEADERS 帧和 DATA 帧，流程与普通响应类似。客户端将这些资源缓存，后续请求时直接使用。

简单来说就是在服务器在推送数据前会先发送一个**PUSH_PROMISE 帧**告诉客户端我要发送数据了，再通过Promised Stream ID告诉客户端发送的数据的流ID，最后发送就好（注意客户端发起的请求用奇数号ID，服务器推送的使用偶数号ID）

## TCP协议
### 总括
首先我们需要知道什么是TCP？通过前面的学习我们知道TCP是一个在传输层的通信协议。那他有什么特点呢？ **面向连接的，可靠的，字节流**。接下来让我们简单理解下这三个特点：

**一、面向连接（Connection-Oriented）**
核心含义
“面向连接” 指通信双方在数据传输前需**先建立逻辑连接**，传输完成后释放连接，类似 “打电话” 的流程（拨号→通话→挂断）。

**实现机制**

**三次握手建立连接**
客户端发送 SYN 包（请求建立连接），携带初始序列号 Seq=x。
服务器回复 SYN+ACK 包（确认请求），携带 Seq=y 和 ACK=x+1（确认客户端序列号）。
客户端回复 ACK 包（确认连接），携带 Seq=x+1 和 ACK=y+1。
连接状态：双方进入 ESTABLISHED 状态，可开始传输数据。

**四次挥手释放连接**
主动关闭方（如客户端）发送 FIN 包（请求关闭连接）。
被动关闭方（服务器）回复 ACK 包（确认关闭请求），继续处理剩余数据。
被动关闭方处理完数据后，发送 FIN 包（正式关闭连接）。
主动关闭方回复 ACK 包（确认关闭），连接彻底释放

**二、可靠传输（Reliable Transmission）**

**核心含义**
TCP 保证数据 **无丢失、无重复、按序**到达，即使网络出现拥塞、分组丢失等问题，也能通过机制修复。

**实现机制**

**序列号（Sequence Number）与确认应答（ACK）**
每个字节数据都有唯一序列号（如 Seq=1000 表示该包第一个字节是数据流中第 1000 字节）。
接收方收到数据后，返回 ACK=Seq+数据长度 表示确认接收（如收到 1000 字节数据，回复 ACK=2000）。
发送方若未收到 ACK，会超时重传未确认的数据（超时时间动态调整，基于 RTT 往返时间）。

**滑动窗口（Sliding Window）**
流量控制：接收方通过窗口大小告知发送方 “当前可接收的数据量”，避免缓冲区溢出。
批量确认：允许接收方累积确认多个数据包（如收到 1000-2000 字节后，统一回复 ACK=2000），减少 ACK 包数量。

**校验和（Checksum）与重传机制**
发送方对数据计算校验和，接收方验证数据完整性，若错误则丢弃并触发重传。
通过 SACK（选择性确认） 机制，接收方可选定需要重传的字节范围（如仅重传丢失的包，而非全部）。

**拥塞控制（Congestion Control）**
通过慢启动（Slow Start）、拥塞避免（Congestion Avoidance）、快重传（Fast Retransmit）等算法，动态调整发送速率，避免网络拥塞导致丢包。

**三、字节流传输（Byte Stream）**
**核心含义**

TCP 将应用层数据视为 无边界的字节流，不保留应用层的消息边界。例如：
应用层调用 1 次 send("hello") 和 1 次 send("world")，TCP 可能合并为 1 个包发送，或拆分为多个包。接收方通过 recv() 读取时，需自行处理字节流的拼接（如通过分隔符识别消息边界）。

**实现机制**

**缓冲区机制**
发送方有 发送缓冲区：应用层数据写入后，由 TCP 决定何时打包发送（如累积一定字节或超时）。
接收方有 接收缓冲区：数据按序存入缓冲区，应用层通过recv()读取时，可能一次读取多个包的数据。

**粘包与拆包问题**
粘包：多个应用层消息被合并为一个 TCP 包（如连续发送 “hello” 和 “world”，可能合并为 “helloworld”）。
拆包：一个应用层消息被拆分为多个 TCP 包（如发送 1000 字节数据，拆分为两个 500 字节的包）。
解决方案：应用层需自定义协议，例如：
固定长度头（如前 4 字节表示消息总长度）。
分隔符（如 HTTP 用\r\n\r\n分隔请求头和请求体）。

最后借助表格简单总结下：

| **特点**       | **解决的核心问题**                | **关键技术**                          | **典型协议/场景**         |
|----------------|-----------------------------------|---------------------------------------|--------------------------|
| 面向连接       | 确保通信双方状态同步              | 三次握手/四次挥手                     | HTTP、FTP                |
| 可靠传输       | 保证数据完整、按序到达            | 序列号/ACK、滑动窗口、重传机制        | 文件传输、数据库同步      |
| 字节流         | 提供无边界的连续数据流            | 缓冲区机制、粘包处理                  | 实时日志、自定义协议通信  |

接下来让我们来看下每个实现机制的细节！

### TCP的连接与断开

#### TCP的连接
首先我们来讲下TCP连接的实现思路及目的，我们知道TCP是为了保证传输的可靠性的，即他需要保证：**在不可靠的网络上建立可靠的双向通信通道**。 这个**可靠**我们可以从以下内容去理解：

1.需要**保证数据包到达**

2.到达之后能够根据传递的报文正确**组装**（我们知道如果报文长度太大是需要分段的，因此当接收到这些分段的数据包时我们还需要组装）

PS:怎么保证正确组装？这里我以拼乐高为例：现在假设A和B两人在一起拼乐高，B提供积木，A进行组装。为了能够正确的拼出理想的模型，他们两人需要知道这盒乐高是从哪里开始拼的，还需要知道哪些积木是**本次的**（比如当B把积木给A结果A上厕所去了，当A重新回来的时候就需要判断哪些积木才是本次需要的）也就是说需要知道这次拼接的积木的**起点**，没有他之后的积木就没有依据（最开始的积木都找不到后面还怎么拼？）。于是A和B在开始前要先确定好这次从哪开始。（到这俩人就确定好了，于是可以开始传递积木）接下来俩人开始合作了，B为了保证自己给积木的顺序是可靠的，他每次传积木前都需要告诉A我这次传的是第几个积木。A为了让B放心，每次都会告诉B自己收到第几个了，这样B就知道A收到了哪些。最终通过这样的通信，A和B就保证了传递的可靠性。接下来回到TCP连接

让我们来看下TCP连接过程中的**核心字段**：
**SYN**（Synchronize Sequence Numbers）：连接请求标志位。
**ACK**（Acknowledgment）：确认标志位。
**Seq**（Sequence Number）：序列号，标识本报文段第一个数据字节的编号。
**ACK Num**（Acknowledgment Number）：确认号，期望接收的下一个数据字节的编号。

接着来看**具体流程**：

1. 第一次握手：客户端发送 SYN 包（Client → Server）这说明客户端**能发**
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/506f4d8e737f432c99fd6c1a6fca00ea.png)

**客户端状态**：从 CLOSED → SYN_SENT。
**含义**：“我想和你建立连接，我的初始序列号是 x，请确认。

2. 第二次握手：服务器发送 SYN+ACK 包（Server → Client）证明服务器**能收能发**
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/46552ec943c94aabbdabff9d308831f2.png)

服务器状态：从 LISTEN → SYN_RCVD。
含义：“我同意建立连接，我的初始序列号是 y，已确认收到你的序列号 x。”

3. 第三次握手：客户端发送 ACK 包（Client → Server）证明客户端**能收**
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3f24e6fb190a47e5896c7e388a86523a.png)
客户端状态：从 SYN_SENT → ESTABLISHED。
服务器状态：从 SYN_RCVD → ESTABLISHED。
含义：“我已收到你的确认，现在我们可以开始传输数据了。”

终于，通过这三次握手两者成功建立了连接并互相确认了对方的初始序列号，终于可以开始通信了！

接下来让我们看看过程中的一些细节：

##### 为什么需要三次握手而不是两次？
**1.避免历史连接**
我们假设现在只有两次握手，那么服务器会在客户端发送SYN报文后回复SYN+ACK，此时已经进入连接。  现在我们假设客户端发送前有一个旧的SYN包，并在本次发送时有一个新的SYN包，如果服务器先收到旧的SYN包则会根据旧SYN包中的初始序列号进行回复并进入连接，因为已经建立连接所以服务端也可以发送数据。最后当客户端接受到ACK后发现不是自己所期望的，于是回复RST报文终止。 这就导致了历史连接错误产生同时服务器还白发送了数据。

这里使用小林大佬的图表示这个过程：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8aaf1ee2f0f3491bae1099acbbca9c98.png)

**2.为了初始化序列号**

在每个SYN报文中都记录了发送方的随机初始序列号，为了保证可靠的初始化序列号同步因此需要对方回复ACK告知，如果只有两次连接最终服务端发送的SYN报文就没有得到确认。

##### 为什么需要初始化序列号，双方的初始化序列号为何不同？

**一、安全需求：防御会话劫持攻击**
1. 序列号预测攻击原理
若客户端和服务器使用相同或可预测的 ISN（如固定值或简单递增），攻击者可通过以下方式劫持会话：
监听网络流量，获取当前连接的 ISN。
伪造 TCP 段，猜测下一个合法序列号，欺骗接收方。

2. 随机化 ISN 的防御机制
客户端和服务器各自生成随机 ISN：
客户端 ISN（c）和服务器 ISN（s）通过独立的随机源生成（如时间戳、进程 ID、熵池），两者无关联。
攻击者**需同时猜测两个随机数**：
成功伪造数据包需同时猜对客户端和服务器的 ISN 及后续序列号，概率极低（约为 2^32 分之一）。

二、可靠性需求：**避免历史连接干扰**
1. 网络延迟导致的旧数据包问题
旧连接的数据包（如 SYN 或数据段）可能因网络拥塞延迟到达，若新连接使用相同 ISN，接收方无法区分新旧数据。

2. 不同 ISN 的过滤机制
序列号比较：
接收方通过比较数据包的序列号与当前连接的 ISN，判断是否属于历史连接。
示例：
旧连接 ISN=1000，延迟数据段 Seq=1050 到达。
新连接 ISN=5000（客户端）和 6000（服务器），接收方发现 1050 < 5000，直接丢弃该数据段。

**对比实验：相同 ISN 导致的问题**
1. 假设场景
客户端和服务器均使用固定 ISN=1000：
旧连接 C1 建立，ISN=1000。
C1 关闭后，延迟数据包（如 Seq=1050）仍在网络中。
新连接 C2 建立，双方再次使用 ISN=1000。
服务器收到延迟包 Seq=1050，误认为是 C2 的合法数据，导致**数据混乱。**

##### 握手丢失怎么办？ 
**1.第一次握手丢失**
当客户端发送SYN报文后就进入SYN_SENT状态，当一段时间后无法收到服务端回复的SYN_ACK报文时就会认为握手信息丢失，于是触发**超时重传**，重传报文的信息和之前都一样。 这个超时时间是定义在**操作系统**中的。并且重发也是有一定次数限制的（不然一直重发很浪费资源！）。这个次数限制由一个：tcp_syn_retries的参数控制

**重传间隔变化**
首次超时时间较短（如 1 秒），后续每次重传间隔指数级增长（如 2 秒、4 秒、8 秒……），总耗时约为 31 秒（1+2+4+8+16=31 秒）。

当最后一次重传后会等待32s，如果还是没有回应最终断开TCP连接。此时服务器是这样的：

服务器的状态：**完全无感知**
1. 未分配连接资源
由于服务器未收到 SYN 包，不会创建 TCP 连接对象，也不会记录任何连接状态（如未进入SYN_RECV状态）。
这意味着：
服务器不会消耗内存、文件描述符等资源。
不会触发服务器端的重传机制（因为没有需要响应的包）。

**第二次握手丢失**
当服务端发送SYN_ACK报文后会进入SYN_RCVD状态。此时由于客户端无法收到ACK报文（服务端发的丢失了），服务端无法收到ACK报文（客户端没收到，不会发）。于是在两端都会触发超时重传:
并当重传次数到达参数设置的时时就断开连接

**第三次握手丢失**
和第二次握手丢失的情况类似，当服务器收不到客户端的ACK报文时于是触发超时重传，到达参数设置上限就断开

##### TCP的半连接和全连接队列
首先来看下什么是半连接队列和全连接队列：

**半连接队列（SYN Queue）：**
服务器收到客户端 SYN 包后，在未完成第三次握手前，临时存储连接信息的队列。
状态：SYN_RECV。

**全连接队列（Accept Queue）：**
服务器收到客户端 ACK 包（第三次握手完成）后，存储已建立连接的队列。
状态：ESTABLISHED，等待应用程序调用accept()获取连接。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3be1c2f4c1fe4926ac9a5e0c4a189fb0.png)
**为什么需要这些队列？**
之前我在学习时就想过TCP连接建立后应用层直接基于他开始通信不就好了，为什么还需要放到队列里等待连接呢？后面学习了RabbitMQ后我发现这其实和异步处理的思路很像，这里先分析全连接队列存在的必要性：

**1. 网络层与应用层的职责差异**
网络层（内核）：
负责快速处理 TCP 报文（如三次握手、超时重传），要求高并发、低延迟。
例：Linux 内核通过netfilter机制在微秒级处理 SYN 包。
应用层（用户进程）：
负责业务逻辑（如数据库查询、文件读写），处理速度慢且不可控。
例：Web 服务器处理 HTTP 请求可能需要数十毫秒甚至秒级。

**2. 直接传递连接的问题**
场景：
若三次握手完成后直接唤醒应用程序，而应用程序正在忙于其他请求（如执行复杂 SQL），则新连接将被阻塞。
后果：
服务器无法及时响应后续 SYN 包，导致客户端重传，浪费网络资源。
攻击者可通过发送大量连接请求，使应用程序持续繁忙，实现拒绝服务攻击。
二、全连接队列的核心作用：**缓冲处理延迟**
是不是感觉和异步处理思路很像！一个处理的快，一个处理的慢。并不需要让快的等待慢的，而是把连接放在队列里等待应用层异步的取出处理就好。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/525b41eccc77491cb8860844d7d72f01.png)
再来看下半连接队列存在的必要性：

**防御 SYN Flood 攻击**

若没有半连接队列，服务器收到 SYN 包后需立即分配完整连接资源（如文件描述符、内存缓冲区）。
攻击者可通过伪造大量 SYN 包耗尽服务器资源。
半连接队列的轻量级存储（仅记录 IP、端口、ISN 等少量信息）允许服务器：限制最大半连接数（通过tcp_max_syn_backlog参数）。快速清理超时未完成的连接（默认 63 秒）。

其实简单来说就是分配TCP连接是需要消耗服务器资源的，半连接队列可以用很少的信息存储这些连接请求。避免有人恶意攻击导致分配大量连接耗尽资源。

##### SYN攻击及其预防
先来看下SYN攻击的逻辑及其效果：

**1. 攻击逻辑**
攻击者伪造大量虚假源 IP的 SYN 包发送给服务器。
服务器回复 SYN+ACK 并创建半连接，存入 SYN 队列。
攻击者不发送 ACK 包，导致半连接超时前持续占用队列空间。
当 SYN 队列满后，服务器拒绝新的合法连接请求。

**2. 攻击效果**
服务器 SYN 队列被占满，无法处理新连接。
CPU 资源被大量 SYN 处理消耗，服务响应缓慢。

那基于这样的攻击逻辑我们可以得出避免SYN攻击的方法：

**增大半连接队列长度**
这其实就是一个治标的方法，既然你攻击是基于打满我的SYN队列让别的请求无法建立连接，那我直接扩大让你不好打满。
 
**减少SYN+ACK重传次数（缩短超时时间）**
这个就是快速清除半连接队列里的数据，重传次数少了断开连接自然就快了

<font color = "00bfff">启用SYN Cookies（关键防御）</font>
这个可以认为是治本的方法，既然你打满了我的SYN队列使我无法建立连接，那我直接跨过半连接队列建立不就行了？具体实现是这样的：

服务器不存储半连接状态，而是通过哈希函数计算 SYN+ACK 中的初始序列号得到一个Cookie值。
当收到客户端ACK 时，验证 ISN 的哈希值是否正确，正确则直接创建全连接。
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3ad62b3dc6f8434d9e941c4593ff0b03.png)


**增大netdev_max_backlog**
这个可以增大可缓存数据包的值，避免因队列满丢弃太多请求
#### TCP的断开
既然TCP连接这么优雅，TCP断开也是如此：
##### 具体过程
**1. 第一次挥手**：客户端发起关闭请求（FIN_WAIT_1 状态）
动作：客户端完成数据发送后，主动向服务端发送 FIN（Finish）报文段，表示 “我已无数据要发送”，但仍可接收服务端数据。
参数：FIN=1，序列号 seq = u（u 为客户端最后一个字节的序列号 + 1）。
状态变化：客户端从 ESTABLISHED（已建立连接） 变为 FIN_WAIT_1（等待第一次确认）。

**2. 第二次挥手**：服务端确认关闭请求（CLOSE_WAIT 状态）
动作：服务端收到客户端的 FIN 后，立即回复 ACK（确认）报文段，表示 “我已收到关闭请求”，但服务端可能仍有数据未发送完毕。
参数：ACK=1，确认号 ack = u+1（确认客户端的 FIN），序列号 seq = v（v 为服务端当前未发送数据的起始序列号）。
状态变化：服务端从 ESTABLISHED 变为 CLOSE_WAIT（等待关闭）。
关键点：此时客户端进入 FIN_WAIT_2（等待第二次确认） 状态，等待服务端发送完剩余数据并发起关闭。

**3. 第三次挥手**：服务端发起关闭请求（LAST_ACK 状态）
动作：服务端发送完所有剩余数据后，向客户端发送 FIN 报文段，表示 “我已无数据要发送，请求关闭连接”。
参数：FIN=1，ACK=1（可与 FIN 合并发送），确认号 ack = u+1（沿用第二次挥手的确认号），序列号 seq = w（w 为服务端最后一个字节的序列号 + 1，需确保与之前发送的数据序列号连续）。
状态变化：服务端从 CLOSE_WAIT 变为 LAST_ACK（等待最后一次确认）。

**4. 第四次挥手**：客户端确认最终关闭（TIME_WAIT 状态）
动作：客户端收到服务端的 FIN 后，回复 ACK 报文段，表示 “我已确认你的关闭请求”。
参数：ACK=1，确认号 ack = w+1（确认服务端的 FIN），序列号 seq = u+1（与第一次挥手的 seq 一致，未发送新数据）。
状态变化：
客户端从 FIN_WAIT_2 变为 TIME_WAIT（超时等待），并启动 2MSL（Maximum Segment Lifetime，最长报文段寿命） 计时器。
服务端收到 ACK 后，从 LAST_ACK 变为 CLOSED（关闭），彻底释放连接资源。
客户端在 TIME_WAIT 状态等待 2MSL 后，若无异常（如未收到服务端重传的 FIN），则变为 CLOSED。

下面使用小林大佬的图来直观的表示：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7d655b6966ae4c1098c62cfd2963d26a.png)
看完我们可以发现，每次发送的FIN报文都是在告知对方**“我已没有数据要发送”**，ACK报文都是在通知对方：**“我已收到，报告给你”**。最终当双方都无数据发送并且收到对方的应答报告后即可关闭连接，因此需要四次挥手。

##### 关键状态解析
<font color = "00bfff">**TIME_WAIT**</font>
我们可以发现，当服务端发送FIN报文给客户端并且客户端回复ACK报文后还经历了一段时间才关闭连接，那为什么不直接关闭而要等待一会呢？
（图里的MSL指的是报文在网络中最大的存活时间）

**1. 确保最后一个 ACK 报文可靠到达**
场景：主动关闭方发送的最后一个 ACK 报文可能在传输中丢失。
问题：若该 ACK 未到达对端（被动关闭方），对端会因未收到确认而重发 FIN 断开请求报文。
**TIME_WAIT 的作用：**
主动关闭方在 TIME_WAIT 状态中保持监听，若接收到对端重发的 FIN 报文，可重新发送 ACK，确保连接正常终止。（这里也可以解释为什么TIME_WAIT 状态需要持续2MSL，这样就可以保证重发的报文在一来一回的过程中正常被确认）。如果没有这个状态，则客户端在收到重传的FIN报文时会直接回复一个RST报文，服务端会将这个RST报文视为**错误信息**，这是我们不愿看到的。

**2. 防止旧连接的重复报文干扰新连接**
背景：网络中可能存在旧连接的延迟报文（称为 延迟分组或 迷途报文）。
问题：若新连接（与旧连接使用相同的源 / 目的 IP 和端口）建立时，旧连接的延迟报文（如携带数据的报文）到达，可能被误认为是新连接的合法数据，导致数据错乱。

这里我们讲一下为什么新连接可能会收到旧连接的信息：因为TCP是由**四元组**唯一确定的，即**源IP,源端口，目标IP，目标端口**。当新旧连接的四元组一致时会被认为是同一次连接。 那之前不是提到过可以用序列号来处理这样的历史连接吗？但是问题在于序列号是有限的，也就是说经过了一段时间后序列号是会**回绕**的，这就会导致无法识别新旧数据。因此我们需要TIME_WAIT这个状态使得旧报文在TIME_WAIT的时间里**全部过期**

**TIME_WAIT 的作用：**
旧连接的所有延迟报文会在 2MSL 时间内自然过期（MSL 是报文在网络中的最大存活时间，2MSL 确保双向路径上的延迟报文都已消失）。
新连接在 TIME_WAIT 结束后建立，可避免旧报文干扰新连接的数据交互。

**TIME_WAIT状态过多的危害及其原因**
首先我们来看下TIME_WAIT状态过多的**危害**：

**1. 占用本地端口资源**
原理：TCP 连接的四元组（源 IP、源端口、目标 IP、目标端口）唯一标识一条连接。客户端主动关闭连接时，会进入TIME_WAIT状态并持续2MSL（Maximum Segment Lifetime，报文最大生存时间，通常为 2 分钟）。在此期间，源端口会被占用，无法被其他新连接复用。
危害：
当短连接（如 HTTP 请求）频繁建立和关闭时，大量端口会被TIME_WAIT状态的连接长期占用。
操作系统对端口的限制（如 Linux 默认端口范围通常为 32768~61000）可能导致端口耗尽，新连接无法建立，影响服务可用性。

**2. 消耗系统资源**
原理：每个TIME_WAIT状态的连接都会在操作系统的 TCP 协议栈中保留连接状态信息（如路由、定时器等）。
危害：
大量TIME_WAIT连接会占用内存资源，可能导致系统内存不足，甚至触发 OOM（Out of Memory）机制，杀死进程。
协议栈需要维护这些连接的定时器（如等待最后一个 ACK 的重传定时器），增加CPU 调度开销，影响系统性能。

接下来来看下这种异常状态的**原因**：
首先我们需要知道TIME_WAIT状态只会存在于**主动关闭方**，那服务器什么时候会出现需要主动关闭连接呢？这主要和HTTP的长连接机制有关：

**1.未开启长连接**
未开启长连接就说明开启的是短连接，在短连接中每次请求/响应完后都会由**服务端主动关闭连接**。在短时间内爆发大量请求时,TIME_WAIT状态就会快速堆积
**2.长连接超时**
为了避免资源浪费（不能建立连接就一直不断吧），nginx会设置一个定时器，当客户端发完最后一条请求后并在这个定时器指定的时间内都没有新的请求，就会由服务器主动关闭连接
**3.长连接请求数量到达上限**
在每个长连接内能处理的请求也是有限的，当到达这个上限值时也会主动关闭连接

<font color = "00bfff">**CLOSE_WAIT**</font>
首先CLOSE_WAIT 状态是在**被动关闭方**出现的。在接收到主动关闭方的FIN报文并回复ACK时被动关闭方就会进入CLOSE_WAIT状态。这个状态的意义在于为被动关闭方处理数据提供时间。（就像在睡觉时突然被喊醒执行任务，但总还是要给你一点时间穿衣服吧）

**CLOSE_WAIT状态过多的原因**
首先我们需要知道，FIN报文的发出是需要调用close函数的，因此我们可以分析发现不调用close就不会发FIN报文，不发FIN报文就会卡在CLOSE_WAIT状态。所以让我们来看下在什么情况下会导致没有调用close函数：

先来看TCP服务端常规流程：

**初始化监听**：创建服务端 socket ，通过 bind 函数绑定特定端口，再用 listen 函数让其处于监听状态，准备接收客户端连接 。

**注册监听事件**：将服务端 socket 注册到 epoll 。epoll 是 Linux 下高效的 I/O 多路复用机制，用于监控多个文件描述符的事件。注册后，就能通过它感知连接相关事件。

**等待并获取连接**：调用 epoll_wait 阻塞等待连接事件到来。当有连接请求时，调用 accept 函数获取已连接的 socket ，代表与客户端建立好的通信通道。

**注册已连接 socket 事件**：把获取到的已连接 socket 再次注册到 epoll ，以便后续监控这个连接上的读写等事件 。
等待事件处理：再次调用 epoll_wait ，等待在已连接 socket 上注册的事件（如可读、可写、异常等）发生，然后根据不同事件进行相应处理 。

**关闭连接**：当检测到对方连接关闭（收到客户端发送的 FIN 报文 ），服务端调用 close 函数关闭对应的 socket 连接，释放相关资源 。
（这里的epoll和RabbitMQ很像，当注册的socket有事件发生时epoll就会感知并通知。这样服务器就不用在轮询了）

基于此我们可以就可以解释没有调用close函数的原因：

**未注册服务端 socket 到 epoll**：若第 2 步未执行，新连接到来时，服务端无法通过 epoll 感知该事件，也就不能调用 accept 获取已连接 socket ，自然没机会后续调用 close 。这属于明显代码逻辑错误，一般在代码检视阶段就能发现。

**未调用 accept 获取连接**：第 3 步没做的话，有新连接时无法获取对应的 socket 。大量客户端主动断开连接，服务端因没获取到 socket ，无法对其调用 close ，导致出现大量 CLOSE_WAIT 状态连接。可能是在执行 accept 前，代码卡在某个逻辑处理中，或者提前抛出异常，中断了正常执行流程。

**未注册已连接 socket 到 epoll**：通过 accept 获取已连接 socket 后，若未执行第 4 步将其注册到 epoll ，后续收到客户端发送的 FIN 报文时，服务端无法感知连接关闭事件，进而无法调用 close 。原因可能是在注册操作前，代码出现逻辑阻塞或异常抛出，阻止了注册操作完成。

**未执行关闭操作**：当客户端关闭连接，服务端理应执行第 6 步调用 close 。若未执行，可能是代码遗漏了关闭处理逻辑；也可能是在执行 close 前，代码陷入死锁等逻辑困境，无法正常执行到关闭函数处。

<font color = "00bfff">**建立连接但崩溃**</font>
这里可分为两种情况：
**1.客户端崩溃**
这里如果服务端向客户端发送数据，则由于客户端不响应触发超时重传，到达上限后发送RST报文关闭连接。
如果一直不发送数据，就会触发**TCP保活机制**（默认关闭，需通过setsockopt()设置SO_KEEPALIVE选项，并且默认时间时两小时），服务器就会发送**保活探测包**：若客户端正常响应（ACK），连接保持活跃。若客户端无响应（网络中断或崩溃），服务器重复发送探测包（默认 9 次，间隔 75 秒）。若所有探测包均无响应，服务器认为连接已失效，关闭连接并通知应用层。

可以发现基于保活机制断开连接是比较耗时的，因此应用层也会实现心跳机制（如之前提到过的长连接超时启动定时器）

**2.服务器崩溃**
当服务器的进程崩溃后会由操作系统回收与TCP连接相关的资源并由内核发送FIN报文与客户端完成四次挥手断开连接

### TCP核心机制
接下来让我们来看下TCP具体是怎么实现面向连接的**可靠传输**的！

#### 重传机制
##### 超时重传
前面我们提到过，在TCP连接中，每发送一个数据都要期望得到对方返回的确认信息。当**一段时间**里无法收到确认信息时就认为发送的数据包丢失于是需要重传。显然无法收到确认信息有两种情况：一是对方压根**没收到数据包**，自然不会回复；二是对方回复了，但**回复信息丢失**。 这时候问题又来了：这个**一段时间到底该是多长呢**？

我们来看小林大佬的这张图：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/cf840ab5c49d4e16ac8295913a323383.png)
我们可以发现：消息从发送到接受本来就是有一段时延的，这段时间叫做RTT。于是我们很容易想到:当重传时间RTO太短，小于RTT时就会导致回复的**包还没到达就启动重传**，这自然是不合理的。但是当RTO太长又没必要，因为在RTT时间后还没收到其实已经说明数据丢失了。所以我们希望RTO的时间时略大于RTT的。 但问题又来了：网络的情况是不固定的，也就导致RTT的值不固定，那我们应该怎样估计RTT的值并计算RTO呢？大致可描述为以下两步：
**1.计算平滑RTT值**
SRTT = (1 - α) × SRTT + α × 新采样RTT
其中，α 为平滑因子，RFC 6289 推荐 α = 0.125（即 1/8）。
初始 SRTT 可设为第一个采样 RTT 值。
**2.计算RTT波动**
RTTVAR = (1 - β) × RTTVAR + β × |新采样RTT - SRTT|
其中，β 为偏差因子，RFC 6289 推荐 β = 0.25（即 1/4）。
初始 RTTVAR 可设为采样 RTT 的一半。

这里使用小林大佬的图简要表示：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8f33313fba7c47529a7b818a70cc708c.png)
超时重传到底还是需要等待的，那有没有更快的方法呢？
##### 快速重传
这里使用小林大佬的图帮大家直观感受：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b1105c093ad04e13aa6548799e1d4113.png)
我们知道：seq代表发送数据的序号，ACK代表期望接受数据的序号。由于Seq2丢失接受方没有收到，于是在之后的回复中都会返回ACK 2。当三次同样ACK到达时就可以在RTO时间之前重发Seq2这个数据。
但这样的机制也有一定问题：
我们假设从Seq2 开始的数据包都丢失了，由于接收方没有收到Seq2就会一直回复ACK2，当触发快速重传的时候就需要考虑这样一个问题：到底是只重发Seq2还是之后的都重发？
##### SACK
这个技术就解决了我们之前留下的问题，他的核心思想就是：既然发送方不知到到底该重发哪些数据，我就把我已经收到的数据告诉你，这样你就可以确认并重发了。还是接着上面的例子，通过SACK，发送方就知道了接收方收到了Seq4之后的消息，于是就可以只重发Seq2和Seq3。但还有个问题，当丢包的时候发送方并不知道到底是发送过去的包丢失还是回复的报文丢失还是网络延迟

##### D-SACK
这个技术又解决了刚刚我们提出的问题，我们来看小林大佬的这两幅图：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/68a64506c4704c24944e166ed185ba17.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/daafcaf9ccdd49eeb539936c48d20daf.png)
在第一幅图中，由于回应的报文丢失触发了超时重发，这时由于前面几次数据都到达接受方，于是触发超时重传，重传3000—3499，但由于之前的乙成功到达接收方，于是接收方回复SACK 3000-3500,并且这时ACK值为4000说明之前的数据已经收到，于是发送方便知道是回复的报文丢失了。

下图的情况也很类似，不过由于其实网络延迟导致的，我们可以发现接收方收到包含1000—1500的D-SACK并非是重传数据包引起的，而是延迟后到达的数据引起的。这就可以说明接受方已收到了重传的数据，最终判断出这个数据未到达接受方是由于网络延迟引起的。
#### 滑动窗口
通过前面的讲解我们知道：TCP是每发送一次数据都需要一次确认应答，也就是只有上一个数据包应答后才会发送下一个。这样的效率是比较低的，于是便引入了**窗口**这个概念。通过指定窗口大小，我们就可以**在窗口大小的范围内连续发送多个数据而无需等待确认应答**。并且窗口还有一个好处，我们知道ACK代表了**连续接收的最高序列号**，那当连续发送的n条数据中有每一条数据的ACK报文丢失也可以通过下一个ACK报文进行确认而不用马上重发。

这里使用小林大佬的图来表示：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1c2a72137abd45fc9bf8bc094ed32696.png)
因为当收到ACK 700时已经说明之前的数据都已收到，所以ACK600丢失也无需重复数据。
既然提到窗口大小，那窗口大小是由谁决定？显然是由接收方来控制的，接受方能处理多少就告诉发送方可以发送多少。
接下来让我们具体分析下这个窗口到底是怎么**滑动**的：

这里使用小林大佬的图来表示：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/afeffed2b05348fdba70fc26f6de185a.png)
这里重点需要关注的是#2中的内容，它代表了已在窗口中发送但还没得到确认的信息。因此当接受方处理数据缓慢时就会导致#2的数据过多最终导致可用窗口为0，此时便无法发送数据。 每当接受方处理完数据时#2状态就会变成#1，此时可用窗口的大小自然会变大。那这样的滑动怎样实现呢？写过滑动窗口的同学可以很自然的想到：使用指针来维护；具体实现如下：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f2eef3bab5fc4621bbd3fbc964d6c27a.png)
其中SND.UNA指向了#2的第一个字节，SND.NXT指向了#3的第一个字节，SND.WND是由接受方通知的滑动窗口大小。观察上图自然可知此时可用窗口大小的计算公式为： SND.WND -(SND.NXT - SND.UNA)。

接下来我们看下接受方的滑动窗口：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1394910acc5f457a8fe298b5d63adb1b.png)
 
同样的接受方的滑动窗口也需要用到指针。这里的RCV.NXT就代表的是期望从接受方收到的下一个数据字节序列号
#### 流量控制
显然，为了保证数据传输的可靠性，当接收方在发送数据的时候必须要考虑接受方的处理能力（不然发一大堆又处理不了最终又只能重传），为了实现这样**发送方根据接收方的实际接收能力控制发送的数据量**的效果便有了**流量控制**

下面结合小林大佬的图先来看下最简单的情况：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4f03ad4cb70d432e9e535507a7e9f095.png)
整个过程可以这样描述：

发送方发送数据，可用窗口变小
接受方接受发送的数据并处理
发送方可用窗口变大

接下来分析以下两种比较复杂的情况（假设窗口大小为360）：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1852181eb5a248f5aaf019fe247299e2.png)
整个过程还是可以这样描述：

发送方发送数据，可用窗口变小
接受方接受发送的数据并处理
发送方可用窗口变大
但此时可以发现由于接受方不能及时的处理数据导致窗口越变越小，最终当接受方再次发送80字节的数据时如果接受方还是不能及时处理就会导致窗口减小到0

还有以下这种情况：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/46aa8016525d437190a710f9560c0701.png)
整个过程和之前还是大致一样的。但问题在于当操作系统繁忙时会直接减少接收缓存，这就会导致后续发送的数据丢包

接下来让我们分析下当发生窗口关闭时会有什么现象：
显然的，当接收方的窗口大小为0时会通知发送方，此时发送方便会停止发送数据并等待接收方的**非0信息**，那如果这个非0信息在传输中丢失便会有如下情况发生：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6af80b5e46684182a4a0572590dfeee3.png)
也就是发送方一直等待接收方告知可以发送数据，接收方一直等待发送方的数据，此时就造成了死锁。那为了解决这样的问题就有了下图的机制：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ced715a29e2e456480461400ff3c81d4.png)
即在一定时间后发送一个探测报文，在收到探测报文时就会回复目前的窗口大小。若不为0就可以发送数据，若为0就继续等待（有些TCP会在三次探测报文都检测到为0时发送报文中断连接）

##### 糊涂窗口综合症
我们直接通过这张图来了解这个现象：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/13e68dd53df3440d913f6e620a1d6b73.png)
这里假设接收方每次只能读取发送的数据的1/3，于是在数据处理过程中就导致接收方窗口越变越小，同时发送方能发送就发送，导致接收方窗口进一步变小...如此循环往复就会导致最终一次只能发送几字节的数据，但问题在于每段数据都要封装TCP+IP头部，这就导致发送几字节的数据却要同时带上几十字节的头部，这就是糊涂窗口综合症。根据发生原理我们也能很快想到解决办法：既然小窗口导致了这样的现象，我避免小窗口的出现不就好了？于是便有了以下两种方案：

**1.接收方不通告小窗口**
**实现逻辑：**
当接收方缓冲区未满时，若收到数据导致缓冲区剩余空间较小（如小于某个阈值，通常为 MSS 的一半或一个 MSS 大小），则暂不发送窗口更新，直到缓冲区空间足够大（如超过阈值）或有数据被应用层读取。
示例：假设 MSS 为 1460 字节，接收方缓冲区剩余空间为 500 字节（小于 MSS 的一半），此时收到数据后不更新窗口，直到空间超过 730 字节或应用层读取数据。

**2.发送方不往小窗口中发送数据**
**实现逻辑**
Nagle 算法：该算法的核心思想是在发送数据时，将小数据段积累到一定大小后再发送，避免网络中充斥着许多小数据块。具体来说，当发送方有数据要发送时，先将数据放入发送缓冲区。如果缓冲区中的数据长度达到一个最大分段大小（MSS），或者缓冲区中所有数据的等待时间超过了一定阈值，就将这些数据封装成一个数据包发送出去。在没有达到上述条件之前，新产生的小数据会被缓存在发送缓冲区，直到满足发送条件。
#### 拥塞控制
通过前面的流量控制我们实现了让发送方根据接收方的处理能力调整自己的发送能力，但网络环境是复杂的，也就是说当网络因为其他因素导致拥塞时就会导致：**发送数据包->数据包时延丢失->重传数据包，占用网络资源->网络进一步拥塞**，为了避免这样的正反馈调节便需要**拥塞控制**。 同时发送方为了避免数据包快速占满网络导致拥塞，便需要**拥塞窗口**（此时发送窗口的值就等于min（拥塞窗口，接收窗口））。 那如何判断拥塞呢？当**发生了超时重传时，就会认为发生了拥塞** （因为超时重传说明了接收方的ACK报文迟迟无法到达），接下来让我们逐一了解实现拥塞控制的算法：

##### 慢启动
他的核心思想就是在连接建立初期或拥塞恢复后，以**较低速率试探**网络容量，避免突发流量引发拥塞。具体实现如下图：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/2f4a429fb2414f40aa90571b6fccc610.png)
其实就是设置初始的拥塞窗口大小为1，当发送方每收到一个ACK时窗口大小+1。于是就会出现这种情况：窗口大小变大->可发送数据变多->窗口进一步变大...最终使得拥塞窗口以**指数级**的增长速度变大。那很明显不能一直变大啊，于是便设置了一个阈值**ssthresh**，当窗口大小<ssthresh时采用慢启动算法，>ssthresh时启动**拥塞避免**算法

##### 拥塞避免
大致流程直接看下图：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/78ed3c3101ac49bb8212c1b744364f1e.png)
具体实现是这样的：当到达ssthresh时拥塞窗口（假设为a）的大小增长就如下：每收到一个ACK时：a += 1/a。例如当窗口大小为8时，8个ACK就使得窗口大小+1，后面变成九个、十个...这样就使得原本指数级的增长速度快速降为**线性的**。但继续增长也不行啊，窗口越大越容易堵啊！

##### 拥塞发生
当窗口大小太大导致数据包重传时，就会启动拥塞发生。这里又根据重传机制分为两种情况：

**1.超时重传**
由于触发超时重传说明好几次回复的ACK报文都消失了，这样的情况是挺严重的，于是便有下图：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e6781ea3a3b742c9ab8a65dac0b8d249.png)
具体来说就是讲ssthresh值设为拥塞窗口的一半，同时直接将拥塞窗口大小置为1重新慢启动。这样的方式确实激进，因为数据流大小突然减少，这就会导致网络卡顿。于是便有下面的情况：

**2.快速重传**
我们知道快速重传是基于ACK报文触发的，既然此时ACK报文还能正常接收，这是就会认为不是很严重。于是将窗口值变为原来的**一半**，同时ssthresh值设为已缩小的窗口值，启动**快速恢复**

##### 快速恢复
我们先直接看图：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c25f47bf86cf4ea6845924e4b0a40614.png)
可以发现使用快速恢复的话拥塞窗口并不会迅速减少，是维护了一个较高的值并在此基础上线性增长，具体来说是这样实现的：

将慢启动阈值设为当前拥塞窗口的一半，确保后续有足够的发送窗口。

调整拥塞窗口：**cwnd = ssthresh + 3**
将拥塞窗口设为新的 ssthresh 加上 3 ，其中 3表示假设接收方已成功接收 3 个后续数据包（基于 3 个重复 ACK），网络仍有处理能力。
重传丢失的数据包：立即重传被重复 ACK 确认的丢失数据包（无需等待超时）。

**2. 处理后续重复 ACK**
每收到一个额外的重复 ACK，发送方将 cwnd 增加 1。
这一机制称为 **“拥塞窗口膨胀（Window Inflation）”**，允许发送方在等待丢失数据包确认的同时，继续发送少量新数据，充分利用网络带宽。

**3. 处理新 ACK**
当发送方收到对丢失数据包的 ** 新确认（New ACK）** 时：
将 cwnd 设置为当前 ssthresh（即cwnd = ssthresh），结束快速恢复阶段。（说明丢失的数据已经收到了，恢复过程结束回归之前的状态）
进入拥塞避免阶段，后续每经过一个 RTT，cwnd 线性增加 1。

# 总结
本篇文章是我在学习完小林大佬的图解计算机网络后个人整理的笔记，其中更多是阐述自己对理论知识的认知，并没有实际抓包观察。并且很多详细难点也没涉及，因此还是推荐大家先去看小林大佬的讲解。如果有不懂的可以看下我的理解，同时也推荐大家学习时多基于问答的方式了解一个知识，这样自己在探索过程中也能有更多体会！我的这篇笔记也是这样出现的，即学习完后我会对相关知识点深入思考，也是在不断的搜索中才更加深刻的体会到其中一些知识的具体实现！最后希望大家都能在不同知识的学习后真正掌握其具体实现！也请大家指出我笔记中不恰当的表达。
下一篇操作系统学习笔记会在几天后写完，大家觉得我写的不错的话也可以继续看！
